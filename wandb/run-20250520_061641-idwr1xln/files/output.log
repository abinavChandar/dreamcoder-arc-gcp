Run ID: idwr1xln
Running on train-set
wandb: WARNING define_metric(summary='best', goal=...) is deprecated and will be removed. Use define_metric(summary='min') or define_metric(summary='max') instead.
Running EC on arc-dev2 @ 2025-05-20 06:16:42.501657 with 4 CPUs and parameters:
	 noConsolidation  =  False
	 solver  =  python
	 biasOptimal  =  False
	 contextual  =  False
	 iterations  =  1
	 enumerationTimeout  =  600
	 reuseRecognition  =  False
	 ensembleSize  =  1
	 recognitionTimeout  =  0
	 helmholtzRatio  =  1.0
	 topK  =  5
	 topk_use_only_likelihood  =  False
	 maximumFrontier  =  10
	 pseudoCounts  =  30.0
	 aic  =  0.1
	 structurePenalty  =  0.1
	 arity  =  3
	 taskBatchSize  =  25
	 taskReranker  =  default
	 storeTaskMetrics  =  True
	 rewriteTaskMetrics  =  False
	 evalset  =  False
	 bothset  =  False
	 task_isolation  =  False
	 evaluationTimeout  =  1.0
	 cuda  =  False

Currently using this much memory: 591765504
Currently using this much memory: 591765504
Using a waking task batch of size: 25
Using experimental Python parallelism with 4 CPUs
Enumerated         0 programs | 4 jobs | 0 CPUs | 25 tasks | 40.0m CPU rem | 10.0m rem | 0.0 avg lb
Enumerated    11,555 programs | 4 jobs | 3 CPUs | 25 tasks | 38.6m CPU rem | 9.6m rem | 14.0 avg lb
Enumerated   106,066 programs | 4 jobs | 3 CPUs | 25 tasks | 27.3m CPU rem | 6.8m rem | 16.0 avg lb
Enumerated   350,900 programs | 4 jobs | 3 CPUs | 25 tasks | 25.9m CPU rem | 6.5m rem | 17.5 avg lb
/home/avaneuros/dreamcoder-arc/venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/avaneuros/dreamcoder-arc/venv/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
Enumerated   466,256 programs | 0 jobs | 3 CPUs | 0 tasks | 0.0m CPU rem | 0.0m rem | nan avg lb
We enumerated this many programs, for each task:
	 [30742, 33037, 33262, 33262, 30742, 33037, 33262, 33262, 30742, 33037, 33262, 33262, 30742, 33037, 33262, 33262, 30742, 33037, 33262, 33262, 30742, 33037, 33262, 33262, 30742]
Tasks to first solution: [('00d62c1b', 42)]
Generative model enumeration results:
HIT 00d62c1b w/ (lambda (fillobj c4 $0)) ; log prior = -10.552970 ; log likelihood = 0.000000
Hits 1/25 tasks
Average description length of a program solving a task: 10.552970 nats
Generative model average:  0 sec.	median: 0 	max: 0 	standard deviation 0
Currently using this much memory: 744882176
Frontiers discovered top down: 1
Total frontiers: 1
Using an ensemble size of 1. Note that we will only store and test on the best recognition model.
Using grammarBuilder GrammarNetwork(
  (logProductions): Linear(in_features=64, out_features=82, bias=True)
)
Currently using this much memory: 747077632
(ID=0): Training a recognition model from 1 frontiers, 100% Helmholtz, feature extractor MikelArcNet.
(ID=0): Got 0 Helmholtz frontiers - random Helmholtz training? : True
(ID=0): Contextual? False
(ID=0): Bias optimal? False
(ID=0): Aux loss? False (n.b. we train a 'auxiliary' classifier anyway - this controls if gradients propagate back to the future extractor)
Sampling 1000 programs from the prior on 4 CPUs...
Got 835/1000 valid samples.
(ID=0): Epoch 1 Loss 14.17
(ID=0): 	vs MDL (w/o neural net) 14.18
(ID=0): 	1 cum grad steps. 0.6 steps/sec | 81-way aux classif loss 0.6941
(ID=0): Epoch 50 Loss 11.86
(ID=0): 	vs MDL (w/o neural net) 12.20
(ID=0): 	50 cum grad steps. 17.4 steps/sec | 81-way aux classif loss 0.6746
(ID=0): Epoch 100 Loss 11.89
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	100 cum grad steps. 24.9 steps/sec | 81-way aux classif loss 0.5993
(ID=0): Epoch 150 Loss 11.89
(ID=0): 	vs MDL (w/o neural net) 13.44
(ID=0): 	150 cum grad steps. 28.4 steps/sec | 81-way aux classif loss 0.5818
(ID=0): Epoch 200 Loss 9.75
(ID=0): 	vs MDL (w/o neural net) 12.39
(ID=0): 	200 cum grad steps. 30.7 steps/sec | 81-way aux classif loss 0.4824
(ID=0): Epoch 250 Loss 9.97
(ID=0): 	vs MDL (w/o neural net) 13.24
(ID=0): 	250 cum grad steps. 32.0 steps/sec | 81-way aux classif loss 0.4286
(ID=0): Epoch 300 Loss 9.26
(ID=0): 	vs MDL (w/o neural net) 12.19
(ID=0): 	300 cum grad steps. 33.2 steps/sec | 81-way aux classif loss 0.3875
(ID=0): Epoch 350 Loss 10.28
(ID=0): 	vs MDL (w/o neural net) 13.36
(ID=0): 	350 cum grad steps. 34.1 steps/sec | 81-way aux classif loss 0.3484
(ID=0): Epoch 400 Loss 10.00
(ID=0): 	vs MDL (w/o neural net) 12.95
(ID=0): 	400 cum grad steps. 35.0 steps/sec | 81-way aux classif loss 0.3134
(ID=0): Epoch 450 Loss 9.44
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	450 cum grad steps. 35.6 steps/sec | 81-way aux classif loss 0.2913
(ID=0): Epoch 500 Loss 9.77
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	500 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.2561
(ID=0): Epoch 550 Loss 10.63
(ID=0): 	vs MDL (w/o neural net) 13.81
(ID=0): 	550 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.2550
(ID=0): Epoch 600 Loss 10.11
(ID=0): 	vs MDL (w/o neural net) 13.68
(ID=0): 	600 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.2390
(ID=0): Epoch 650 Loss 9.75
(ID=0): 	vs MDL (w/o neural net) 13.20
(ID=0): 	650 cum grad steps. 37.2 steps/sec | 81-way aux classif loss 0.2302
(ID=0): Epoch 700 Loss 9.70
(ID=0): 	vs MDL (w/o neural net) 12.72
(ID=0): 	700 cum grad steps. 37.4 steps/sec | 81-way aux classif loss 0.2123
(ID=0): Epoch 750 Loss 9.17
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	750 cum grad steps. 37.6 steps/sec | 81-way aux classif loss 0.2041
(ID=0): Epoch 800 Loss 9.90
(ID=0): 	vs MDL (w/o neural net) 13.38
(ID=0): 	800 cum grad steps. 37.8 steps/sec | 81-way aux classif loss 0.1965
Sampling 1000 programs from the prior on 4 CPUs...
Got 854/1000 valid samples.
(ID=0): Epoch 850 Loss 9.89
(ID=0): 	vs MDL (w/o neural net) 13.69
(ID=0): 	850 cum grad steps. 33.9 steps/sec | 81-way aux classif loss 0.2059
(ID=0): Epoch 900 Loss 9.19
(ID=0): 	vs MDL (w/o neural net) 12.72
(ID=0): 	900 cum grad steps. 34.3 steps/sec | 81-way aux classif loss 0.1868
(ID=0): Epoch 950 Loss 9.02
(ID=0): 	vs MDL (w/o neural net) 12.28
(ID=0): 	950 cum grad steps. 34.5 steps/sec | 81-way aux classif loss 0.1795
(ID=0): Epoch 1000 Loss 9.43
(ID=0): 	vs MDL (w/o neural net) 12.74
(ID=0): 	1000 cum grad steps. 34.7 steps/sec | 81-way aux classif loss 0.1744
(ID=0): Epoch 1050 Loss 9.69
(ID=0): 	vs MDL (w/o neural net) 12.95
(ID=0): 	1050 cum grad steps. 34.9 steps/sec | 81-way aux classif loss 0.1909
(ID=0): Epoch 1100 Loss 9.38
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	1100 cum grad steps. 35.2 steps/sec | 81-way aux classif loss 0.1692
(ID=0): Epoch 1150 Loss 9.57
(ID=0): 	vs MDL (w/o neural net) 13.18
(ID=0): 	1150 cum grad steps. 35.4 steps/sec | 81-way aux classif loss 0.1783
(ID=0): Epoch 1200 Loss 9.68
(ID=0): 	vs MDL (w/o neural net) 13.25
(ID=0): 	1200 cum grad steps. 35.6 steps/sec | 81-way aux classif loss 0.1643
(ID=0): Epoch 1250 Loss 9.18
(ID=0): 	vs MDL (w/o neural net) 12.72
(ID=0): 	1250 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1652
(ID=0): Epoch 1300 Loss 9.25
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	1300 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1958
(ID=0): Epoch 1350 Loss 9.23
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	1350 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1576
(ID=0): Epoch 1400 Loss 9.08
(ID=0): 	vs MDL (w/o neural net) 12.58
(ID=0): 	1400 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1587
(ID=0): Epoch 1450 Loss 9.42
(ID=0): 	vs MDL (w/o neural net) 12.97
(ID=0): 	1450 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1625
(ID=0): Epoch 1500 Loss 8.55
(ID=0): 	vs MDL (w/o neural net) 12.12
(ID=0): 	1500 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1407
(ID=0): Epoch 1550 Loss 9.28
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	1550 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1445
(ID=0): Epoch 1600 Loss 8.86
(ID=0): 	vs MDL (w/o neural net) 12.37
(ID=0): 	1600 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1481
(ID=0): Epoch 1650 Loss 9.09
(ID=0): 	vs MDL (w/o neural net) 13.04
(ID=0): 	1650 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1423
Sampling 1000 programs from the prior on 4 CPUs...
Got 854/1000 valid samples.
(ID=0): Epoch 1700 Loss 9.11
(ID=0): 	vs MDL (w/o neural net) 13.22
(ID=0): 	1700 cum grad steps. 35.6 steps/sec | 81-way aux classif loss 0.1388
(ID=0): Epoch 1750 Loss 9.01
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	1750 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1634
(ID=0): Epoch 1800 Loss 9.06
(ID=0): 	vs MDL (w/o neural net) 12.30
(ID=0): 	1800 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1372
(ID=0): Epoch 1850 Loss 8.91
(ID=0): 	vs MDL (w/o neural net) 12.74
(ID=0): 	1850 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1410
(ID=0): Epoch 1900 Loss 9.20
(ID=0): 	vs MDL (w/o neural net) 12.80
(ID=0): 	1900 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1449
(ID=0): Epoch 1950 Loss 8.75
(ID=0): 	vs MDL (w/o neural net) 12.38
(ID=0): 	1950 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1337
(ID=0): Epoch 2000 Loss 9.78
(ID=0): 	vs MDL (w/o neural net) 13.41
(ID=0): 	2000 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1450
(ID=0): Epoch 2050 Loss 10.21
(ID=0): 	vs MDL (w/o neural net) 13.82
(ID=0): 	2050 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1517
(ID=0): Epoch 2100 Loss 9.26
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	2100 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1435
(ID=0): Epoch 2150 Loss 8.89
(ID=0): 	vs MDL (w/o neural net) 12.68
(ID=0): 	2150 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1325
(ID=0): Epoch 2200 Loss 8.85
(ID=0): 	vs MDL (w/o neural net) 12.39
(ID=0): 	2200 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1314
(ID=0): Epoch 2250 Loss 9.12
(ID=0): 	vs MDL (w/o neural net) 13.15
(ID=0): 	2250 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1260
(ID=0): Epoch 2300 Loss 9.04
(ID=0): 	vs MDL (w/o neural net) 13.07
(ID=0): 	2300 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1439
(ID=0): Epoch 2350 Loss 8.43
(ID=0): 	vs MDL (w/o neural net) 12.21
(ID=0): 	2350 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1221
(ID=0): Epoch 2400 Loss 9.15
(ID=0): 	vs MDL (w/o neural net) 12.66
(ID=0): 	2400 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1311
(ID=0): Epoch 2450 Loss 9.25
(ID=0): 	vs MDL (w/o neural net) 12.85
(ID=0): 	2450 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1294
(ID=0): Epoch 2500 Loss 8.86
(ID=0): 	vs MDL (w/o neural net) 12.77
(ID=0): 	2500 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1322
Sampling 1000 programs from the prior on 4 CPUs...
Got 859/1000 valid samples.
(ID=0): Epoch 2550 Loss 8.71
(ID=0): 	vs MDL (w/o neural net) 11.86
(ID=0): 	2550 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1253
(ID=0): Epoch 2600 Loss 8.93
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	2600 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1298
(ID=0): Epoch 2650 Loss 9.05
(ID=0): 	vs MDL (w/o neural net) 12.66
(ID=0): 	2650 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1330
(ID=0): Epoch 2700 Loss 8.21
(ID=0): 	vs MDL (w/o neural net) 12.03
(ID=0): 	2700 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1219
(ID=0): Epoch 2750 Loss 9.23
(ID=0): 	vs MDL (w/o neural net) 12.75
(ID=0): 	2750 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1342
(ID=0): Epoch 2800 Loss 8.81
(ID=0): 	vs MDL (w/o neural net) 12.60
(ID=0): 	2800 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1301
(ID=0): Epoch 2850 Loss 9.01
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	2850 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1270
(ID=0): Epoch 2900 Loss 9.20
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	2900 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1272
(ID=0): Epoch 2950 Loss 9.12
(ID=0): 	vs MDL (w/o neural net) 13.45
(ID=0): 	2950 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1313
(ID=0): Epoch 3000 Loss 9.88
(ID=0): 	vs MDL (w/o neural net) 13.31
(ID=0): 	3000 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1333
(ID=0): Epoch 3050 Loss 8.82
(ID=0): 	vs MDL (w/o neural net) 13.17
(ID=0): 	3050 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1221
(ID=0): Epoch 3100 Loss 9.02
(ID=0): 	vs MDL (w/o neural net) 12.75
(ID=0): 	3100 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1217
(ID=0): Epoch 3150 Loss 8.62
(ID=0): 	vs MDL (w/o neural net) 12.53
(ID=0): 	3150 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1223
(ID=0): Epoch 3200 Loss 8.67
(ID=0): 	vs MDL (w/o neural net) 12.71
(ID=0): 	3200 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1319
(ID=0): Epoch 3250 Loss 8.97
(ID=0): 	vs MDL (w/o neural net) 12.66
(ID=0): 	3250 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1363
(ID=0): Epoch 3300 Loss 9.23
(ID=0): 	vs MDL (w/o neural net) 12.98
(ID=0): 	3300 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1426
(ID=0): Epoch 3350 Loss 9.18
(ID=0): 	vs MDL (w/o neural net) 12.84
(ID=0): 	3350 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1442
(ID=0): Epoch 3400 Loss 8.96
(ID=0): 	vs MDL (w/o neural net) 13.08
(ID=0): 	3400 cum grad steps. 37.0 steps/sec | 81-way aux classif loss 0.1990
Sampling 1000 programs from the prior on 4 CPUs...
Got 848/1000 valid samples.
(ID=0): Epoch 3450 Loss 9.23
(ID=0): 	vs MDL (w/o neural net) 13.18
(ID=0): 	3450 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1436
(ID=0): Epoch 3500 Loss 8.90
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	3500 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1320
(ID=0): Epoch 3550 Loss 8.53
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	3550 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1275
(ID=0): Epoch 3600 Loss 9.25
(ID=0): 	vs MDL (w/o neural net) 13.22
(ID=0): 	3600 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1476
(ID=0): Epoch 3650 Loss 8.39
(ID=0): 	vs MDL (w/o neural net) 12.37
(ID=0): 	3650 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1425
(ID=0): Epoch 3700 Loss 8.46
(ID=0): 	vs MDL (w/o neural net) 12.71
(ID=0): 	3700 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1313
(ID=0): Epoch 3750 Loss 9.80
(ID=0): 	vs MDL (w/o neural net) 14.14
(ID=0): 	3750 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1345
(ID=0): Epoch 3800 Loss 8.51
(ID=0): 	vs MDL (w/o neural net) 13.22
(ID=0): 	3800 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1257
(ID=0): Epoch 3850 Loss 8.59
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	3850 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1333
(ID=0): Epoch 3900 Loss 9.22
(ID=0): 	vs MDL (w/o neural net) 13.71
(ID=0): 	3900 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1268
(ID=0): Epoch 3950 Loss 8.98
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	3950 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1271
(ID=0): Epoch 4000 Loss 9.19
(ID=0): 	vs MDL (w/o neural net) 13.31
(ID=0): 	4000 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1343
(ID=0): Epoch 4050 Loss 8.74
(ID=0): 	vs MDL (w/o neural net) 12.54
(ID=0): 	4050 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1397
(ID=0): Epoch 4100 Loss 9.26
(ID=0): 	vs MDL (w/o neural net) 13.38
(ID=0): 	4100 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1370
(ID=0): Epoch 4150 Loss 8.30
(ID=0): 	vs MDL (w/o neural net) 12.36
(ID=0): 	4150 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1385
(ID=0): Epoch 4200 Loss 8.20
(ID=0): 	vs MDL (w/o neural net) 12.25
(ID=0): 	4200 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1256
(ID=0): Epoch 4250 Loss 9.29
(ID=0): 	vs MDL (w/o neural net) 13.10
(ID=0): 	4250 cum grad steps. 36.9 steps/sec | 81-way aux classif loss 0.1263
Sampling 1000 programs from the prior on 4 CPUs...
Got 849/1000 valid samples.
(ID=0): Epoch 4300 Loss 8.29
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	4300 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1299
(ID=0): Epoch 4350 Loss 8.71
(ID=0): 	vs MDL (w/o neural net) 12.80
(ID=0): 	4350 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1300
(ID=0): Epoch 4400 Loss 8.75
(ID=0): 	vs MDL (w/o neural net) 13.14
(ID=0): 	4400 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1417
(ID=0): Epoch 4450 Loss 8.34
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	4450 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1328
(ID=0): Epoch 4500 Loss 8.65
(ID=0): 	vs MDL (w/o neural net) 12.80
(ID=0): 	4500 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1533
(ID=0): Epoch 4550 Loss 8.80
(ID=0): 	vs MDL (w/o neural net) 13.35
(ID=0): 	4550 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1289
(ID=0): Epoch 4600 Loss 8.61
(ID=0): 	vs MDL (w/o neural net) 12.76
(ID=0): 	4600 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1303
(ID=0): Epoch 4650 Loss 8.44
(ID=0): 	vs MDL (w/o neural net) 12.74
(ID=0): 	4650 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1244
(ID=0): Epoch 4700 Loss 8.66
(ID=0): 	vs MDL (w/o neural net) 13.10
(ID=0): 	4700 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1307
(ID=0): Epoch 4750 Loss 7.29
(ID=0): 	vs MDL (w/o neural net) 11.81
(ID=0): 	4750 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1352
(ID=0): Epoch 4800 Loss 9.27
(ID=0): 	vs MDL (w/o neural net) 13.52
(ID=0): 	4800 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1311
(ID=0): Epoch 4850 Loss 8.23
(ID=0): 	vs MDL (w/o neural net) 12.41
(ID=0): 	4850 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1220
(ID=0): Epoch 4900 Loss 8.13
(ID=0): 	vs MDL (w/o neural net) 12.09
(ID=0): 	4900 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1195
(ID=0): Epoch 4950 Loss 8.56
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	4950 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1493
(ID=0): Epoch 5000 Loss 8.08
(ID=0): 	vs MDL (w/o neural net) 12.25
(ID=0): 	5000 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1256
(ID=0): Epoch 5050 Loss 8.55
(ID=0): 	vs MDL (w/o neural net) 12.77
(ID=0): 	5050 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1170
Sampling 1000 programs from the prior on 4 CPUs...
Got 861/1000 valid samples.
(ID=0): Epoch 5100 Loss 9.09
(ID=0): 	vs MDL (w/o neural net) 13.15
(ID=0): 	5100 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1247
(ID=0): Epoch 5150 Loss 7.83
(ID=0): 	vs MDL (w/o neural net) 12.25
(ID=0): 	5150 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1204
(ID=0): Epoch 5200 Loss 8.24
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	5200 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1210
(ID=0): Epoch 5250 Loss 8.83
(ID=0): 	vs MDL (w/o neural net) 13.12
(ID=0): 	5250 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1494
(ID=0): Epoch 5300 Loss 8.49
(ID=0): 	vs MDL (w/o neural net) 12.91
(ID=0): 	5300 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1385
(ID=0): Epoch 5350 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	5350 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1396
(ID=0): Epoch 5400 Loss 8.73
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	5400 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1361
(ID=0): Epoch 5450 Loss 8.18
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	5450 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1210
(ID=0): Epoch 5500 Loss 8.65
(ID=0): 	vs MDL (w/o neural net) 13.08
(ID=0): 	5500 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1212
(ID=0): Epoch 5550 Loss 9.04
(ID=0): 	vs MDL (w/o neural net) 13.66
(ID=0): 	5550 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1244
(ID=0): Epoch 5600 Loss 9.41
(ID=0): 	vs MDL (w/o neural net) 13.96
(ID=0): 	5600 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1321
(ID=0): Epoch 5650 Loss 8.95
(ID=0): 	vs MDL (w/o neural net) 13.30
(ID=0): 	5650 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1310
(ID=0): Epoch 5700 Loss 8.40
(ID=0): 	vs MDL (w/o neural net) 12.92
(ID=0): 	5700 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1306
(ID=0): Epoch 5750 Loss 8.26
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	5750 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1175
(ID=0): Epoch 5800 Loss 8.25
(ID=0): 	vs MDL (w/o neural net) 12.77
(ID=0): 	5800 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1327
(ID=0): Epoch 5850 Loss 8.61
(ID=0): 	vs MDL (w/o neural net) 12.99
(ID=0): 	5850 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1243
(ID=0): Epoch 5900 Loss 8.50
(ID=0): 	vs MDL (w/o neural net) 13.16
(ID=0): 	5900 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1219
(ID=0): Epoch 5950 Loss 8.52
(ID=0): 	vs MDL (w/o neural net) 12.76
(ID=0): 	5950 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1201
Sampling 1000 programs from the prior on 4 CPUs...
Got 830/1000 valid samples.
(ID=0): Epoch 6000 Loss 8.21
(ID=0): 	vs MDL (w/o neural net) 12.50
(ID=0): 	6000 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1287
(ID=0): Epoch 6050 Loss 9.20
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	6050 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1305
(ID=0): Epoch 6100 Loss 8.49
(ID=0): 	vs MDL (w/o neural net) 12.83
(ID=0): 	6100 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1276
(ID=0): Epoch 6150 Loss 8.66
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	6150 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1263
(ID=0): Epoch 6200 Loss 8.20
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	6200 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1211
(ID=0): Epoch 6250 Loss 8.09
(ID=0): 	vs MDL (w/o neural net) 12.36
(ID=0): 	6250 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1191
(ID=0): Epoch 6300 Loss 8.74
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	6300 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1275
(ID=0): Epoch 6350 Loss 8.97
(ID=0): 	vs MDL (w/o neural net) 13.67
(ID=0): 	6350 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1291
(ID=0): Epoch 6400 Loss 8.81
(ID=0): 	vs MDL (w/o neural net) 13.30
(ID=0): 	6400 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1281
(ID=0): Epoch 6450 Loss 8.36
(ID=0): 	vs MDL (w/o neural net) 13.26
(ID=0): 	6450 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1221
(ID=0): Epoch 6500 Loss 8.61
(ID=0): 	vs MDL (w/o neural net) 12.85
(ID=0): 	6500 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1247
(ID=0): Epoch 6550 Loss 8.73
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	6550 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1207
(ID=0): Epoch 6600 Loss 7.47
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	6600 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1438
(ID=0): Epoch 6650 Loss 8.07
(ID=0): 	vs MDL (w/o neural net) 12.36
(ID=0): 	6650 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1182
(ID=0): Epoch 6700 Loss 8.61
(ID=0): 	vs MDL (w/o neural net) 13.56
(ID=0): 	6700 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1229
(ID=0): Epoch 6750 Loss 8.80
(ID=0): 	vs MDL (w/o neural net) 13.12
(ID=0): 	6750 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1304
Sampling 1000 programs from the prior on 4 CPUs...
Got 856/1000 valid samples.
(ID=0): Epoch 6800 Loss 8.36
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	6800 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1313
(ID=0): Epoch 6850 Loss 7.96
(ID=0): 	vs MDL (w/o neural net) 12.18
(ID=0): 	6850 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1267
(ID=0): Epoch 6900 Loss 8.23
(ID=0): 	vs MDL (w/o neural net) 12.66
(ID=0): 	6900 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1270
(ID=0): Epoch 6950 Loss 8.64
(ID=0): 	vs MDL (w/o neural net) 12.84
(ID=0): 	6950 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1314
(ID=0): Epoch 7000 Loss 8.09
(ID=0): 	vs MDL (w/o neural net) 12.16
(ID=0): 	7000 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1231
(ID=0): Epoch 7050 Loss 9.10
(ID=0): 	vs MDL (w/o neural net) 13.68
(ID=0): 	7050 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1242
(ID=0): Epoch 7100 Loss 8.17
(ID=0): 	vs MDL (w/o neural net) 12.64
(ID=0): 	7100 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1214
(ID=0): Epoch 7150 Loss 8.59
(ID=0): 	vs MDL (w/o neural net) 12.65
(ID=0): 	7150 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1283
(ID=0): Epoch 7200 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	7200 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1214
(ID=0): Epoch 7250 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 12.38
(ID=0): 	7250 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1183
(ID=0): Epoch 7300 Loss 7.77
(ID=0): 	vs MDL (w/o neural net) 12.20
(ID=0): 	7300 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1418
(ID=0): Epoch 7350 Loss 8.76
(ID=0): 	vs MDL (w/o neural net) 13.62
(ID=0): 	7350 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1216
(ID=0): Epoch 7400 Loss 8.31
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	7400 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1162
(ID=0): Epoch 7450 Loss 8.59
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	7450 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1218
(ID=0): Epoch 7500 Loss 9.05
(ID=0): 	vs MDL (w/o neural net) 13.41
(ID=0): 	7500 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1283
(ID=0): Epoch 7550 Loss 7.98
(ID=0): 	vs MDL (w/o neural net) 12.39
(ID=0): 	7550 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1111
(ID=0): Epoch 7600 Loss 8.29
(ID=0): 	vs MDL (w/o neural net) 12.63
(ID=0): 	7600 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1179
Sampling 1000 programs from the prior on 4 CPUs...
Got 861/1000 valid samples.
(ID=0): Epoch 7650 Loss 8.06
(ID=0): 	vs MDL (w/o neural net) 12.58
(ID=0): 	7650 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1238
(ID=0): Epoch 7700 Loss 8.41
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	7700 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1252
(ID=0): Epoch 7750 Loss 9.09
(ID=0): 	vs MDL (w/o neural net) 13.38
(ID=0): 	7750 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1330
(ID=0): Epoch 7800 Loss 7.47
(ID=0): 	vs MDL (w/o neural net) 11.83
(ID=0): 	7800 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1379
(ID=0): Epoch 7850 Loss 8.60
(ID=0): 	vs MDL (w/o neural net) 12.93
(ID=0): 	7850 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1216
(ID=0): Epoch 7900 Loss 6.99
(ID=0): 	vs MDL (w/o neural net) 11.46
(ID=0): 	7900 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1113
(ID=0): Epoch 7950 Loss 9.14
(ID=0): 	vs MDL (w/o neural net) 13.65
(ID=0): 	7950 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1252
(ID=0): Epoch 8000 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 12.24
(ID=0): 	8000 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1189
(ID=0): Epoch 8050 Loss 8.57
(ID=0): 	vs MDL (w/o neural net) 12.97
(ID=0): 	8050 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1171
(ID=0): Epoch 8100 Loss 8.57
(ID=0): 	vs MDL (w/o neural net) 13.12
(ID=0): 	8100 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1225
(ID=0): Epoch 8150 Loss 8.26
(ID=0): 	vs MDL (w/o neural net) 13.06
(ID=0): 	8150 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1182
(ID=0): Epoch 8200 Loss 8.50
(ID=0): 	vs MDL (w/o neural net) 12.67
(ID=0): 	8200 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1198
(ID=0): Epoch 8250 Loss 8.35
(ID=0): 	vs MDL (w/o neural net) 13.17
(ID=0): 	8250 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1224
(ID=0): Epoch 8300 Loss 8.49
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	8300 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1215
(ID=0): Epoch 8350 Loss 8.71
(ID=0): 	vs MDL (w/o neural net) 12.93
(ID=0): 	8350 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1272
(ID=0): Epoch 8400 Loss 8.33
(ID=0): 	vs MDL (w/o neural net) 12.56
(ID=0): 	8400 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1170
(ID=0): Epoch 8450 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 12.41
(ID=0): 	8450 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1192
(ID=0): Epoch 8500 Loss 8.69
(ID=0): 	vs MDL (w/o neural net) 13.10
(ID=0): 	8500 cum grad steps. 36.8 steps/sec | 81-way aux classif loss 0.1272
Sampling 1000 programs from the prior on 4 CPUs...
Got 840/1000 valid samples.
(ID=0): Epoch 8550 Loss 8.74
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	8550 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1245
(ID=0): Epoch 8600 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.05
(ID=0): 	8600 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1220
(ID=0): Epoch 8650 Loss 8.91
(ID=0): 	vs MDL (w/o neural net) 13.61
(ID=0): 	8650 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1294
(ID=0): Epoch 8700 Loss 8.08
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	8700 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1155
(ID=0): Epoch 8750 Loss 7.11
(ID=0): 	vs MDL (w/o neural net) 11.72
(ID=0): 	8750 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1096
(ID=0): Epoch 8800 Loss 8.13
(ID=0): 	vs MDL (w/o neural net) 12.38
(ID=0): 	8800 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1159
(ID=0): Epoch 8850 Loss 8.20
(ID=0): 	vs MDL (w/o neural net) 12.57
(ID=0): 	8850 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1188
(ID=0): Epoch 8900 Loss 8.23
(ID=0): 	vs MDL (w/o neural net) 12.69
(ID=0): 	8900 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1256
(ID=0): Epoch 8950 Loss 8.46
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	8950 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1257
(ID=0): Epoch 9000 Loss 8.52
(ID=0): 	vs MDL (w/o neural net) 13.15
(ID=0): 	9000 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1215
(ID=0): Epoch 9050 Loss 7.72
(ID=0): 	vs MDL (w/o neural net) 12.01
(ID=0): 	9050 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1135
(ID=0): Epoch 9100 Loss 8.42
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	9100 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1244
(ID=0): Epoch 9150 Loss 7.53
(ID=0): 	vs MDL (w/o neural net) 12.54
(ID=0): 	9150 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1077
(ID=0): Epoch 9200 Loss 8.59
(ID=0): 	vs MDL (w/o neural net) 13.25
(ID=0): 	9200 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1163
(ID=0): Epoch 9250 Loss 8.65
(ID=0): 	vs MDL (w/o neural net) 13.17
(ID=0): 	9250 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1221
(ID=0): Epoch 9300 Loss 7.77
(ID=0): 	vs MDL (w/o neural net) 12.19
(ID=0): 	9300 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1164
Sampling 1000 programs from the prior on 4 CPUs...
Got 864/1000 valid samples.
(ID=0): Epoch 9350 Loss 8.53
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	9350 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1241
(ID=0): Epoch 9400 Loss 8.81
(ID=0): 	vs MDL (w/o neural net) 13.33
(ID=0): 	9400 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1246
(ID=0): Epoch 9450 Loss 8.36
(ID=0): 	vs MDL (w/o neural net) 12.99
(ID=0): 	9450 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1267
(ID=0): Epoch 9500 Loss 9.69
(ID=0): 	vs MDL (w/o neural net) 14.25
(ID=0): 	9500 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1270
(ID=0): Epoch 9550 Loss 8.26
(ID=0): 	vs MDL (w/o neural net) 13.07
(ID=0): 	9550 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1222
(ID=0): Epoch 9600 Loss 7.99
(ID=0): 	vs MDL (w/o neural net) 12.73
(ID=0): 	9600 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1158
(ID=0): Epoch 9650 Loss 8.55
(ID=0): 	vs MDL (w/o neural net) 13.05
(ID=0): 	9650 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1238
(ID=0): Epoch 9700 Loss 8.46
(ID=0): 	vs MDL (w/o neural net) 12.66
(ID=0): 	9700 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1360
(ID=0): Epoch 9750 Loss 7.64
(ID=0): 	vs MDL (w/o neural net) 12.14
(ID=0): 	9750 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1188
(ID=0): Epoch 9800 Loss 8.78
(ID=0): 	vs MDL (w/o neural net) 13.31
(ID=0): 	9800 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1305
(ID=0): Epoch 9850 Loss 7.66
(ID=0): 	vs MDL (w/o neural net) 11.95
(ID=0): 	9850 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1209
(ID=0): Epoch 9900 Loss 8.11
(ID=0): 	vs MDL (w/o neural net) 12.68
(ID=0): 	9900 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1217
(ID=0): Epoch 9950 Loss 7.86
(ID=0): 	vs MDL (w/o neural net) 12.63
(ID=0): 	9950 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1243
(ID=0): Epoch 10000 Loss 8.66
(ID=0): 	vs MDL (w/o neural net) 13.17
(ID=0): 	10000 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1260
(ID=0): Epoch 10050 Loss 8.66
(ID=0): 	vs MDL (w/o neural net) 13.49
(ID=0): 	10050 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1241
(ID=0): Epoch 10100 Loss 8.54
(ID=0): 	vs MDL (w/o neural net) 13.28
(ID=0): 	10100 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1246
(ID=0): Epoch 10150 Loss 8.30
(ID=0): 	vs MDL (w/o neural net) 13.10
(ID=0): 	10150 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1418
(ID=0): Epoch 10200 Loss 8.45
(ID=0): 	vs MDL (w/o neural net) 13.11
(ID=0): 	10200 cum grad steps. 36.7 steps/sec | 81-way aux classif loss 0.1165
Sampling 1000 programs from the prior on 4 CPUs...
Got 868/1000 valid samples.
(ID=0): Epoch 10250 Loss 7.45
(ID=0): 	vs MDL (w/o neural net) 12.26
(ID=0): 	10250 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1228
(ID=0): Epoch 10300 Loss 7.58
(ID=0): 	vs MDL (w/o neural net) 12.47
(ID=0): 	10300 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1140
(ID=0): Epoch 10350 Loss 8.34
(ID=0): 	vs MDL (w/o neural net) 13.33
(ID=0): 	10350 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1215
(ID=0): Epoch 10400 Loss 8.22
(ID=0): 	vs MDL (w/o neural net) 13.01
(ID=0): 	10400 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1216
(ID=0): Epoch 10450 Loss 8.36
(ID=0): 	vs MDL (w/o neural net) 13.01
(ID=0): 	10450 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1214
(ID=0): Epoch 10500 Loss 8.21
(ID=0): 	vs MDL (w/o neural net) 13.05
(ID=0): 	10500 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1154
(ID=0): Epoch 10550 Loss 7.84
(ID=0): 	vs MDL (w/o neural net) 12.71
(ID=0): 	10550 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1130
(ID=0): Epoch 10600 Loss 7.92
(ID=0): 	vs MDL (w/o neural net) 12.65
(ID=0): 	10600 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1137
(ID=0): Epoch 10650 Loss 8.48
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	10650 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1173
(ID=0): Epoch 10700 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.05
(ID=0): 	10700 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1186
(ID=0): Epoch 10750 Loss 7.54
(ID=0): 	vs MDL (w/o neural net) 12.13
(ID=0): 	10750 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1091
(ID=0): Epoch 10800 Loss 8.44
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	10800 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1144
(ID=0): Epoch 10850 Loss 7.52
(ID=0): 	vs MDL (w/o neural net) 12.24
(ID=0): 	10850 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1080
(ID=0): Epoch 10900 Loss 7.98
(ID=0): 	vs MDL (w/o neural net) 12.85
(ID=0): 	10900 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1123
(ID=0): Epoch 10950 Loss 7.35
(ID=0): 	vs MDL (w/o neural net) 11.93
(ID=0): 	10950 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1052
(ID=0): Epoch 11000 Loss 8.12
(ID=0): 	vs MDL (w/o neural net) 12.92
(ID=0): 	11000 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1167
(ID=0): Epoch 11050 Loss 7.62
(ID=0): 	vs MDL (w/o neural net) 12.39
(ID=0): 	11050 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1161
Sampling 1000 programs from the prior on 4 CPUs...
Got 852/1000 valid samples.
(ID=0): Epoch 11100 Loss 7.81
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	11100 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1166
(ID=0): Epoch 11150 Loss 7.95
(ID=0): 	vs MDL (w/o neural net) 12.69
(ID=0): 	11150 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1169
(ID=0): Epoch 11200 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 12.79
(ID=0): 	11200 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1159
(ID=0): Epoch 11250 Loss 7.78
(ID=0): 	vs MDL (w/o neural net) 12.15
(ID=0): 	11250 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1147
(ID=0): Epoch 11300 Loss 8.54
(ID=0): 	vs MDL (w/o neural net) 13.19
(ID=0): 	11300 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1162
(ID=0): Epoch 11350 Loss 7.97
(ID=0): 	vs MDL (w/o neural net) 12.73
(ID=0): 	11350 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1137
(ID=0): Epoch 11400 Loss 8.74
(ID=0): 	vs MDL (w/o neural net) 13.97
(ID=0): 	11400 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1190
(ID=0): Epoch 11450 Loss 8.64
(ID=0): 	vs MDL (w/o neural net) 13.08
(ID=0): 	11450 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1232
(ID=0): Epoch 11500 Loss 8.30
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	11500 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1216
(ID=0): Epoch 11550 Loss 8.88
(ID=0): 	vs MDL (w/o neural net) 13.42
(ID=0): 	11550 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1209
(ID=0): Epoch 11600 Loss 8.62
(ID=0): 	vs MDL (w/o neural net) 13.39
(ID=0): 	11600 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1306
(ID=0): Epoch 11650 Loss 8.15
(ID=0): 	vs MDL (w/o neural net) 12.92
(ID=0): 	11650 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1160
(ID=0): Epoch 11700 Loss 8.09
(ID=0): 	vs MDL (w/o neural net) 13.13
(ID=0): 	11700 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1133
(ID=0): Epoch 11750 Loss 7.77
(ID=0): 	vs MDL (w/o neural net) 12.56
(ID=0): 	11750 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1082
(ID=0): Epoch 11800 Loss 8.66
(ID=0): 	vs MDL (w/o neural net) 13.16
(ID=0): 	11800 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1179
(ID=0): Epoch 11850 Loss 7.89
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	11850 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1112
(ID=0): Epoch 11900 Loss 7.63
(ID=0): 	vs MDL (w/o neural net) 12.61
(ID=0): 	11900 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1137
Sampling 1000 programs from the prior on 4 CPUs...
Got 866/1000 valid samples.
(ID=0): Epoch 11950 Loss 8.43
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	11950 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1160
(ID=0): Epoch 12000 Loss 8.43
(ID=0): 	vs MDL (w/o neural net) 13.21
(ID=0): 	12000 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1125
(ID=0): Epoch 12050 Loss 7.73
(ID=0): 	vs MDL (w/o neural net) 12.67
(ID=0): 	12050 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1141
(ID=0): Epoch 12100 Loss 7.98
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	12100 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1141
(ID=0): Epoch 12150 Loss 7.70
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	12150 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1111
(ID=0): Epoch 12200 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 12.93
(ID=0): 	12200 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1130
(ID=0): Epoch 12250 Loss 8.31
(ID=0): 	vs MDL (w/o neural net) 13.07
(ID=0): 	12250 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1175
(ID=0): Epoch 12300 Loss 7.74
(ID=0): 	vs MDL (w/o neural net) 12.43
(ID=0): 	12300 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1148
(ID=0): Epoch 12350 Loss 7.73
(ID=0): 	vs MDL (w/o neural net) 12.67
(ID=0): 	12350 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1129
(ID=0): Epoch 12400 Loss 7.50
(ID=0): 	vs MDL (w/o neural net) 12.83
(ID=0): 	12400 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1145
(ID=0): Epoch 12450 Loss 8.10
(ID=0): 	vs MDL (w/o neural net) 13.04
(ID=0): 	12450 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1127
(ID=0): Epoch 12500 Loss 8.53
(ID=0): 	vs MDL (w/o neural net) 13.34
(ID=0): 	12500 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1202
(ID=0): Epoch 12550 Loss 7.81
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	12550 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1102
(ID=0): Epoch 12600 Loss 7.42
(ID=0): 	vs MDL (w/o neural net) 12.27
(ID=0): 	12600 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1028
(ID=0): Epoch 12650 Loss 7.51
(ID=0): 	vs MDL (w/o neural net) 12.10
(ID=0): 	12650 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1084
(ID=0): Epoch 12700 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 13.16
(ID=0): 	12700 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1181
(ID=0): Epoch 12750 Loss 7.92
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	12750 cum grad steps. 36.6 steps/sec | 81-way aux classif loss 0.1139
Sampling 1000 programs from the prior on 4 CPUs...
Got 833/1000 valid samples.
(ID=0): Epoch 12800 Loss 8.22
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	12800 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1195
(ID=0): Epoch 12850 Loss 7.76
(ID=0): 	vs MDL (w/o neural net) 12.41
(ID=0): 	12850 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1168
(ID=0): Epoch 12900 Loss 8.17
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	12900 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1156
(ID=0): Epoch 12950 Loss 9.14
(ID=0): 	vs MDL (w/o neural net) 14.03
(ID=0): 	12950 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1234
(ID=0): Epoch 13000 Loss 8.25
(ID=0): 	vs MDL (w/o neural net) 13.11
(ID=0): 	13000 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1161
(ID=0): Epoch 13050 Loss 8.26
(ID=0): 	vs MDL (w/o neural net) 12.95
(ID=0): 	13050 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1210
(ID=0): Epoch 13100 Loss 7.92
(ID=0): 	vs MDL (w/o neural net) 12.58
(ID=0): 	13100 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1141
(ID=0): Epoch 13150 Loss 8.15
(ID=0): 	vs MDL (w/o neural net) 12.40
(ID=0): 	13150 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1138
(ID=0): Epoch 13200 Loss 7.81
(ID=0): 	vs MDL (w/o neural net) 12.53
(ID=0): 	13200 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1115
(ID=0): Epoch 13250 Loss 6.81
(ID=0): 	vs MDL (w/o neural net) 11.86
(ID=0): 	13250 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1051
(ID=0): Epoch 13300 Loss 8.22
(ID=0): 	vs MDL (w/o neural net) 13.10
(ID=0): 	13300 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1171
(ID=0): Epoch 13350 Loss 7.95
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	13350 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1164
(ID=0): Epoch 13400 Loss 8.53
(ID=0): 	vs MDL (w/o neural net) 13.25
(ID=0): 	13400 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1200
(ID=0): Epoch 13450 Loss 8.72
(ID=0): 	vs MDL (w/o neural net) 13.43
(ID=0): 	13450 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1157
(ID=0): Epoch 13500 Loss 7.54
(ID=0): 	vs MDL (w/o neural net) 12.30
(ID=0): 	13500 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1125
(ID=0): Epoch 13550 Loss 7.97
(ID=0): 	vs MDL (w/o neural net) 12.94
(ID=0): 	13550 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1107
(ID=0): Epoch 13600 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	13600 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1138
Sampling 1000 programs from the prior on 4 CPUs...
Got 830/1000 valid samples.
(ID=0): Epoch 13650 Loss 8.55
(ID=0): 	vs MDL (w/o neural net) 13.17
(ID=0): 	13650 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1160
(ID=0): Epoch 13700 Loss 7.65
(ID=0): 	vs MDL (w/o neural net) 11.85
(ID=0): 	13700 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1101
(ID=0): Epoch 13750 Loss 7.75
(ID=0): 	vs MDL (w/o neural net) 12.46
(ID=0): 	13750 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1127
(ID=0): Epoch 13800 Loss 9.31
(ID=0): 	vs MDL (w/o neural net) 13.59
(ID=0): 	13800 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1348
(ID=0): Epoch 13850 Loss 7.89
(ID=0): 	vs MDL (w/o neural net) 12.83
(ID=0): 	13850 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1082
(ID=0): Epoch 13900 Loss 7.50
(ID=0): 	vs MDL (w/o neural net) 13.01
(ID=0): 	13900 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1067
(ID=0): Epoch 13950 Loss 8.69
(ID=0): 	vs MDL (w/o neural net) 13.18
(ID=0): 	13950 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1225
(ID=0): Epoch 14000 Loss 8.08
(ID=0): 	vs MDL (w/o neural net) 13.13
(ID=0): 	14000 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1256
(ID=0): Epoch 14050 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 13.06
(ID=0): 	14050 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1185
(ID=0): Epoch 14100 Loss 8.30
(ID=0): 	vs MDL (w/o neural net) 12.99
(ID=0): 	14100 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1183
(ID=0): Epoch 14150 Loss 7.88
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	14150 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1205
(ID=0): Epoch 14200 Loss 7.94
(ID=0): 	vs MDL (w/o neural net) 12.59
(ID=0): 	14200 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1084
(ID=0): Epoch 14250 Loss 7.34
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	14250 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1132
(ID=0): Epoch 14300 Loss 9.09
(ID=0): 	vs MDL (w/o neural net) 13.68
(ID=0): 	14300 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1258
(ID=0): Epoch 14350 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 13.07
(ID=0): 	14350 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1195
(ID=0): Epoch 14400 Loss 7.24
(ID=0): 	vs MDL (w/o neural net) 12.29
(ID=0): 	14400 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1159
(ID=0): Epoch 14450 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.62
(ID=0): 	14450 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1209
Sampling 1000 programs from the prior on 4 CPUs...
Got 859/1000 valid samples.
(ID=0): Epoch 14500 Loss 7.98
(ID=0): 	vs MDL (w/o neural net) 12.76
(ID=0): 	14500 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1217
(ID=0): Epoch 14550 Loss 8.26
(ID=0): 	vs MDL (w/o neural net) 12.88
(ID=0): 	14550 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1172
(ID=0): Epoch 14600 Loss 7.85
(ID=0): 	vs MDL (w/o neural net) 13.15
(ID=0): 	14600 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1136
(ID=0): Epoch 14650 Loss 8.30
(ID=0): 	vs MDL (w/o neural net) 13.54
(ID=0): 	14650 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1200
(ID=0): Epoch 14700 Loss 7.91
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	14700 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1174
(ID=0): Epoch 14750 Loss 8.03
(ID=0): 	vs MDL (w/o neural net) 13.28
(ID=0): 	14750 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1243
(ID=0): Epoch 14800 Loss 8.61
(ID=0): 	vs MDL (w/o neural net) 13.51
(ID=0): 	14800 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1360
(ID=0): Epoch 14850 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 12.91
(ID=0): 	14850 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1333
(ID=0): Epoch 14900 Loss 8.45
(ID=0): 	vs MDL (w/o neural net) 13.47
(ID=0): 	14900 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1268
(ID=0): Epoch 14950 Loss 8.12
(ID=0): 	vs MDL (w/o neural net) 13.21
(ID=0): 	14950 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1248
(ID=0): Epoch 15000 Loss 7.85
(ID=0): 	vs MDL (w/o neural net) 12.84
(ID=0): 	15000 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1350
(ID=0): Epoch 15050 Loss 8.48
(ID=0): 	vs MDL (w/o neural net) 13.62
(ID=0): 	15050 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1344
(ID=0): Epoch 15100 Loss 8.13
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	15100 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1183
(ID=0): Epoch 15150 Loss 7.87
(ID=0): 	vs MDL (w/o neural net) 13.23
(ID=0): 	15150 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1203
(ID=0): Epoch 15200 Loss 8.03
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	15200 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1337
(ID=0): Epoch 15250 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 12.84
(ID=0): 	15250 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1284
(ID=0): Epoch 15300 Loss 8.14
(ID=0): 	vs MDL (w/o neural net) 13.38
(ID=0): 	15300 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1161
Sampling 1000 programs from the prior on 4 CPUs...
Got 831/1000 valid samples.
(ID=0): Epoch 15350 Loss 8.56
(ID=0): 	vs MDL (w/o neural net) 13.26
(ID=0): 	15350 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1297
(ID=0): Epoch 15400 Loss 8.75
(ID=0): 	vs MDL (w/o neural net) 13.44
(ID=0): 	15400 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1244
(ID=0): Epoch 15450 Loss 7.63
(ID=0): 	vs MDL (w/o neural net) 12.78
(ID=0): 	15450 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1195
(ID=0): Epoch 15500 Loss 7.61
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	15500 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1108
(ID=0): Epoch 15550 Loss 8.14
(ID=0): 	vs MDL (w/o neural net) 12.29
(ID=0): 	15550 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1152
(ID=0): Epoch 15600 Loss 8.22
(ID=0): 	vs MDL (w/o neural net) 13.23
(ID=0): 	15600 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1229
(ID=0): Epoch 15650 Loss 8.46
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	15650 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1186
(ID=0): Epoch 15700 Loss 8.42
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	15700 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1151
(ID=0): Epoch 15750 Loss 9.50
(ID=0): 	vs MDL (w/o neural net) 14.64
(ID=0): 	15750 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1327
(ID=0): Epoch 15800 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	15800 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1260
(ID=0): Epoch 15850 Loss 7.66
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	15850 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1155
(ID=0): Epoch 15900 Loss 8.24
(ID=0): 	vs MDL (w/o neural net) 13.42
(ID=0): 	15900 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1212
(ID=0): Epoch 15950 Loss 7.79
(ID=0): 	vs MDL (w/o neural net) 12.80
(ID=0): 	15950 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1163
(ID=0): Epoch 16000 Loss 9.07
(ID=0): 	vs MDL (w/o neural net) 13.04
(ID=0): 	16000 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1321
(ID=0): Epoch 16050 Loss 7.74
(ID=0): 	vs MDL (w/o neural net) 12.19
(ID=0): 	16050 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1122
(ID=0): Epoch 16100 Loss 8.58
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	16100 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1371
(ID=0): Epoch 16150 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 13.58
(ID=0): 	16150 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1284
Sampling 1000 programs from the prior on 4 CPUs...
Got 840/1000 valid samples.
(ID=0): Epoch 16200 Loss 7.89
(ID=0): 	vs MDL (w/o neural net) 12.58
(ID=0): 	16200 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1209
(ID=0): Epoch 16250 Loss 7.42
(ID=0): 	vs MDL (w/o neural net) 12.31
(ID=0): 	16250 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1222
(ID=0): Epoch 16300 Loss 8.07
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	16300 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1260
(ID=0): Epoch 16350 Loss 7.40
(ID=0): 	vs MDL (w/o neural net) 12.23
(ID=0): 	16350 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1363
(ID=0): Epoch 16400 Loss 8.24
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	16400 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1455
(ID=0): Epoch 16450 Loss 8.06
(ID=0): 	vs MDL (w/o neural net) 12.67
(ID=0): 	16450 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1200
(ID=0): Epoch 16500 Loss 8.51
(ID=0): 	vs MDL (w/o neural net) 13.08
(ID=0): 	16500 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1291
(ID=0): Epoch 16550 Loss 7.71
(ID=0): 	vs MDL (w/o neural net) 12.57
(ID=0): 	16550 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1244
(ID=0): Epoch 16600 Loss 7.05
(ID=0): 	vs MDL (w/o neural net) 12.25
(ID=0): 	16600 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1223
(ID=0): Epoch 16650 Loss 8.78
(ID=0): 	vs MDL (w/o neural net) 13.72
(ID=0): 	16650 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1467
(ID=0): Epoch 16700 Loss 8.02
(ID=0): 	vs MDL (w/o neural net) 12.95
(ID=0): 	16700 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1148
(ID=0): Epoch 16750 Loss 7.90
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	16750 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1256
(ID=0): Epoch 16800 Loss 8.95
(ID=0): 	vs MDL (w/o neural net) 13.57
(ID=0): 	16800 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1366
(ID=0): Epoch 16850 Loss 8.23
(ID=0): 	vs MDL (w/o neural net) 13.23
(ID=0): 	16850 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1347
(ID=0): Epoch 16900 Loss 7.78
(ID=0): 	vs MDL (w/o neural net) 12.98
(ID=0): 	16900 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1287
(ID=0): Epoch 16950 Loss 7.85
(ID=0): 	vs MDL (w/o neural net) 12.72
(ID=0): 	16950 cum grad steps. 36.5 steps/sec | 81-way aux classif loss 0.1178
Sampling 1000 programs from the prior on 4 CPUs...
Got 859/1000 valid samples.
(ID=0): Epoch 17000 Loss 6.74
(ID=0): 	vs MDL (w/o neural net) 11.70
(ID=0): 	17000 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1094
(ID=0): Epoch 17050 Loss 7.49
(ID=0): 	vs MDL (w/o neural net) 12.26
(ID=0): 	17050 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1219
(ID=0): Epoch 17100 Loss 8.00
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	17100 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1185
(ID=0): Epoch 17150 Loss 7.96
(ID=0): 	vs MDL (w/o neural net) 12.64
(ID=0): 	17150 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1136
(ID=0): Epoch 17200 Loss 8.12
(ID=0): 	vs MDL (w/o neural net) 13.51
(ID=0): 	17200 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1172
(ID=0): Epoch 17250 Loss 8.58
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	17250 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1268
(ID=0): Epoch 17300 Loss 8.00
(ID=0): 	vs MDL (w/o neural net) 12.91
(ID=0): 	17300 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1104
(ID=0): Epoch 17350 Loss 7.76
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	17350 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1094
(ID=0): Epoch 17400 Loss 7.34
(ID=0): 	vs MDL (w/o neural net) 12.79
(ID=0): 	17400 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1116
(ID=0): Epoch 17450 Loss 7.60
(ID=0): 	vs MDL (w/o neural net) 12.67
(ID=0): 	17450 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1051
(ID=0): Epoch 17500 Loss 8.32
(ID=0): 	vs MDL (w/o neural net) 13.07
(ID=0): 	17500 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1162
(ID=0): Epoch 17550 Loss 7.78
(ID=0): 	vs MDL (w/o neural net) 12.93
(ID=0): 	17550 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1108
(ID=0): Epoch 17600 Loss 8.75
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	17600 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1202
(ID=0): Epoch 17650 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 13.04
(ID=0): 	17650 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1203
(ID=0): Epoch 17700 Loss 8.50
(ID=0): 	vs MDL (w/o neural net) 13.49
(ID=0): 	17700 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1235
(ID=0): Epoch 17750 Loss 7.49
(ID=0): 	vs MDL (w/o neural net) 12.39
(ID=0): 	17750 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1027
(ID=0): Epoch 17800 Loss 8.09
(ID=0): 	vs MDL (w/o neural net) 13.05
(ID=0): 	17800 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1080
Sampling 1000 programs from the prior on 4 CPUs...
Got 846/1000 valid samples.
(ID=0): Epoch 17850 Loss 7.97
(ID=0): 	vs MDL (w/o neural net) 12.71
(ID=0): 	17850 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1126
(ID=0): Epoch 17900 Loss 7.68
(ID=0): 	vs MDL (w/o neural net) 12.46
(ID=0): 	17900 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1071
(ID=0): Epoch 17950 Loss 8.48
(ID=0): 	vs MDL (w/o neural net) 13.51
(ID=0): 	17950 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1189
(ID=0): Epoch 18000 Loss 8.44
(ID=0): 	vs MDL (w/o neural net) 13.40
(ID=0): 	18000 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1141
(ID=0): Epoch 18050 Loss 7.84
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	18050 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1142
(ID=0): Epoch 18100 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 13.19
(ID=0): 	18100 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1374
(ID=0): Epoch 18150 Loss 7.89
(ID=0): 	vs MDL (w/o neural net) 12.45
(ID=0): 	18150 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1119
(ID=0): Epoch 18200 Loss 8.48
(ID=0): 	vs MDL (w/o neural net) 13.79
(ID=0): 	18200 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1196
(ID=0): Epoch 18250 Loss 7.57
(ID=0): 	vs MDL (w/o neural net) 12.72
(ID=0): 	18250 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1099
(ID=0): Epoch 18300 Loss 7.10
(ID=0): 	vs MDL (w/o neural net) 11.73
(ID=0): 	18300 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1024
(ID=0): Epoch 18350 Loss 7.71
(ID=0): 	vs MDL (w/o neural net) 12.27
(ID=0): 	18350 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1145
(ID=0): Epoch 18400 Loss 7.52
(ID=0): 	vs MDL (w/o neural net) 12.36
(ID=0): 	18400 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1045
(ID=0): Epoch 18450 Loss 7.79
(ID=0): 	vs MDL (w/o neural net) 12.53
(ID=0): 	18450 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1125
(ID=0): Epoch 18500 Loss 7.14
(ID=0): 	vs MDL (w/o neural net) 12.15
(ID=0): 	18500 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.0990
(ID=0): Epoch 18550 Loss 8.67
(ID=0): 	vs MDL (w/o neural net) 13.59
(ID=0): 	18550 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1190
(ID=0): Epoch 18600 Loss 8.32
(ID=0): 	vs MDL (w/o neural net) 13.36
(ID=0): 	18600 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1119
(ID=0): Epoch 18650 Loss 7.96
(ID=0): 	vs MDL (w/o neural net) 12.98
(ID=0): 	18650 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1128
Sampling 1000 programs from the prior on 4 CPUs...
Got 865/1000 valid samples.
(ID=0): Epoch 18700 Loss 8.14
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	18700 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1218
(ID=0): Epoch 18750 Loss 7.52
(ID=0): 	vs MDL (w/o neural net) 12.63
(ID=0): 	18750 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1034
(ID=0): Epoch 18800 Loss 8.24
(ID=0): 	vs MDL (w/o neural net) 13.35
(ID=0): 	18800 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1118
(ID=0): Epoch 18850 Loss 7.33
(ID=0): 	vs MDL (w/o neural net) 11.99
(ID=0): 	18850 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1010
(ID=0): Epoch 18900 Loss 8.45
(ID=0): 	vs MDL (w/o neural net) 12.99
(ID=0): 	18900 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1184
(ID=0): Epoch 18950 Loss 8.33
(ID=0): 	vs MDL (w/o neural net) 13.43
(ID=0): 	18950 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1181
(ID=0): Epoch 19000 Loss 8.30
(ID=0): 	vs MDL (w/o neural net) 13.06
(ID=0): 	19000 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1138
(ID=0): Epoch 19050 Loss 6.74
(ID=0): 	vs MDL (w/o neural net) 12.04
(ID=0): 	19050 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0993
(ID=0): Epoch 19100 Loss 8.73
(ID=0): 	vs MDL (w/o neural net) 13.71
(ID=0): 	19100 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1212
(ID=0): Epoch 19150 Loss 7.46
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	19150 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0981
(ID=0): Epoch 19200 Loss 7.44
(ID=0): 	vs MDL (w/o neural net) 12.36
(ID=0): 	19200 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0993
(ID=0): Epoch 19250 Loss 7.22
(ID=0): 	vs MDL (w/o neural net) 12.60
(ID=0): 	19250 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0995
(ID=0): Epoch 19300 Loss 7.19
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	19300 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1012
(ID=0): Epoch 19350 Loss 8.07
(ID=0): 	vs MDL (w/o neural net) 12.93
(ID=0): 	19350 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1098
(ID=0): Epoch 19400 Loss 7.22
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	19400 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1050
(ID=0): Epoch 19450 Loss 7.80
(ID=0): 	vs MDL (w/o neural net) 12.48
(ID=0): 	19450 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1124
(ID=0): Epoch 19500 Loss 8.20
(ID=0): 	vs MDL (w/o neural net) 13.48
(ID=0): 	19500 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1156
(ID=0): Epoch 19550 Loss 7.95
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	19550 cum grad steps. 36.4 steps/sec | 81-way aux classif loss 0.1122
Sampling 1000 programs from the prior on 4 CPUs...
Got 826/1000 valid samples.
(ID=0): Epoch 19600 Loss 7.69
(ID=0): 	vs MDL (w/o neural net) 12.47
(ID=0): 	19600 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1131
(ID=0): Epoch 19650 Loss 8.45
(ID=0): 	vs MDL (w/o neural net) 13.78
(ID=0): 	19650 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1132
(ID=0): Epoch 19700 Loss 7.69
(ID=0): 	vs MDL (w/o neural net) 12.41
(ID=0): 	19700 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1063
(ID=0): Epoch 19750 Loss 7.46
(ID=0): 	vs MDL (w/o neural net) 12.26
(ID=0): 	19750 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1034
(ID=0): Epoch 19800 Loss 7.95
(ID=0): 	vs MDL (w/o neural net) 13.08
(ID=0): 	19800 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1113
(ID=0): Epoch 19850 Loss 8.23
(ID=0): 	vs MDL (w/o neural net) 13.59
(ID=0): 	19850 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1126
(ID=0): Epoch 19900 Loss 6.42
(ID=0): 	vs MDL (w/o neural net) 11.25
(ID=0): 	19900 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0952
(ID=0): Epoch 19950 Loss 7.57
(ID=0): 	vs MDL (w/o neural net) 12.46
(ID=0): 	19950 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1112
(ID=0): Epoch 20000 Loss 7.48
(ID=0): 	vs MDL (w/o neural net) 12.20
(ID=0): 	20000 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1051
(ID=0): Epoch 20050 Loss 8.09
(ID=0): 	vs MDL (w/o neural net) 13.52
(ID=0): 	20050 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1054
(ID=0): Epoch 20100 Loss 7.77
(ID=0): 	vs MDL (w/o neural net) 12.78
(ID=0): 	20100 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1061
(ID=0): Epoch 20150 Loss 7.96
(ID=0): 	vs MDL (w/o neural net) 13.15
(ID=0): 	20150 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1140
(ID=0): Epoch 20200 Loss 7.41
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	20200 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1010
(ID=0): Epoch 20250 Loss 8.47
(ID=0): 	vs MDL (w/o neural net) 13.01
(ID=0): 	20250 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1167
(ID=0): Epoch 20300 Loss 7.22
(ID=0): 	vs MDL (w/o neural net) 12.37
(ID=0): 	20300 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1084
(ID=0): Epoch 20350 Loss 7.97
(ID=0): 	vs MDL (w/o neural net) 12.99
(ID=0): 	20350 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1100
Sampling 1000 programs from the prior on 4 CPUs...
Got 837/1000 valid samples.
(ID=0): Epoch 20400 Loss 7.53
(ID=0): 	vs MDL (w/o neural net) 12.76
(ID=0): 	20400 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1061
(ID=0): Epoch 20450 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	20450 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1077
(ID=0): Epoch 20500 Loss 7.90
(ID=0): 	vs MDL (w/o neural net) 13.20
(ID=0): 	20500 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1100
(ID=0): Epoch 20550 Loss 8.89
(ID=0): 	vs MDL (w/o neural net) 13.64
(ID=0): 	20550 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1138
(ID=0): Epoch 20600 Loss 7.32
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	20600 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1048
(ID=0): Epoch 20650 Loss 7.70
(ID=0): 	vs MDL (w/o neural net) 12.85
(ID=0): 	20650 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1089
(ID=0): Epoch 20700 Loss 8.06
(ID=0): 	vs MDL (w/o neural net) 13.24
(ID=0): 	20700 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1099
(ID=0): Epoch 20750 Loss 7.43
(ID=0): 	vs MDL (w/o neural net) 12.60
(ID=0): 	20750 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1032
(ID=0): Epoch 20800 Loss 7.55
(ID=0): 	vs MDL (w/o neural net) 12.57
(ID=0): 	20800 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1016
(ID=0): Epoch 20850 Loss 8.07
(ID=0): 	vs MDL (w/o neural net) 12.74
(ID=0): 	20850 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1113
(ID=0): Epoch 20900 Loss 8.45
(ID=0): 	vs MDL (w/o neural net) 13.31
(ID=0): 	20900 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1134
(ID=0): Epoch 20950 Loss 7.99
(ID=0): 	vs MDL (w/o neural net) 13.26
(ID=0): 	20950 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1096
(ID=0): Epoch 21000 Loss 7.60
(ID=0): 	vs MDL (w/o neural net) 12.42
(ID=0): 	21000 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1047
(ID=0): Epoch 21050 Loss 7.33
(ID=0): 	vs MDL (w/o neural net) 12.43
(ID=0): 	21050 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1046
(ID=0): Epoch 21100 Loss 7.68
(ID=0): 	vs MDL (w/o neural net) 12.59
(ID=0): 	21100 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1097
(ID=0): Epoch 21150 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	21150 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1092
(ID=0): Epoch 21200 Loss 7.21
(ID=0): 	vs MDL (w/o neural net) 12.60
(ID=0): 	21200 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1094
Sampling 1000 programs from the prior on 4 CPUs...
Got 850/1000 valid samples.
(ID=0): Epoch 21250 Loss 7.29
(ID=0): 	vs MDL (w/o neural net) 12.23
(ID=0): 	21250 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0982
(ID=0): Epoch 21300 Loss 7.89
(ID=0): 	vs MDL (w/o neural net) 13.11
(ID=0): 	21300 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1062
(ID=0): Epoch 21350 Loss 7.92
(ID=0): 	vs MDL (w/o neural net) 12.94
(ID=0): 	21350 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1054
(ID=0): Epoch 21400 Loss 8.14
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	21400 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1092
(ID=0): Epoch 21450 Loss 7.40
(ID=0): 	vs MDL (w/o neural net) 12.16
(ID=0): 	21450 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1003
(ID=0): Epoch 21500 Loss 7.62
(ID=0): 	vs MDL (w/o neural net) 12.30
(ID=0): 	21500 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1024
(ID=0): Epoch 21550 Loss 6.96
(ID=0): 	vs MDL (w/o neural net) 11.94
(ID=0): 	21550 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1048
(ID=0): Epoch 21600 Loss 8.35
(ID=0): 	vs MDL (w/o neural net) 13.42
(ID=0): 	21600 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1095
(ID=0): Epoch 21650 Loss 7.32
(ID=0): 	vs MDL (w/o neural net) 12.13
(ID=0): 	21650 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1022
(ID=0): Epoch 21700 Loss 8.23
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	21700 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1136
(ID=0): Epoch 21750 Loss 8.01
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	21750 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1100
(ID=0): Epoch 21800 Loss 7.16
(ID=0): 	vs MDL (w/o neural net) 12.42
(ID=0): 	21800 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1042
(ID=0): Epoch 21850 Loss 7.59
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	21850 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1053
(ID=0): Epoch 21900 Loss 8.56
(ID=0): 	vs MDL (w/o neural net) 13.82
(ID=0): 	21900 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1113
(ID=0): Epoch 21950 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	21950 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1059
(ID=0): Epoch 22000 Loss 8.18
(ID=0): 	vs MDL (w/o neural net) 13.58
(ID=0): 	22000 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1114
(ID=0): Epoch 22050 Loss 7.58
(ID=0): 	vs MDL (w/o neural net) 12.28
(ID=0): 	22050 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1048
Sampling 1000 programs from the prior on 4 CPUs...
Got 854/1000 valid samples.
(ID=0): Epoch 22100 Loss 7.21
(ID=0): 	vs MDL (w/o neural net) 12.35
(ID=0): 	22100 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1022
(ID=0): Epoch 22150 Loss 7.71
(ID=0): 	vs MDL (w/o neural net) 12.85
(ID=0): 	22150 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1044
(ID=0): Epoch 22200 Loss 8.12
(ID=0): 	vs MDL (w/o neural net) 13.44
(ID=0): 	22200 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1131
(ID=0): Epoch 22250 Loss 7.50
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	22250 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1064
(ID=0): Epoch 22300 Loss 7.51
(ID=0): 	vs MDL (w/o neural net) 12.71
(ID=0): 	22300 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1086
(ID=0): Epoch 22350 Loss 8.01
(ID=0): 	vs MDL (w/o neural net) 13.30
(ID=0): 	22350 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1128
(ID=0): Epoch 22400 Loss 7.36
(ID=0): 	vs MDL (w/o neural net) 12.57
(ID=0): 	22400 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1133
(ID=0): Epoch 22450 Loss 7.60
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	22450 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1032
(ID=0): Epoch 22500 Loss 8.00
(ID=0): 	vs MDL (w/o neural net) 13.31
(ID=0): 	22500 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1097
(ID=0): Epoch 22550 Loss 7.81
(ID=0): 	vs MDL (w/o neural net) 13.33
(ID=0): 	22550 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1136
(ID=0): Epoch 22600 Loss 7.99
(ID=0): 	vs MDL (w/o neural net) 12.97
(ID=0): 	22600 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1068
(ID=0): Epoch 22650 Loss 8.35
(ID=0): 	vs MDL (w/o neural net) 13.45
(ID=0): 	22650 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1155
(ID=0): Epoch 22700 Loss 7.78
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	22700 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1074
(ID=0): Epoch 22750 Loss 8.27
(ID=0): 	vs MDL (w/o neural net) 12.97
(ID=0): 	22750 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1158
(ID=0): Epoch 22800 Loss 8.34
(ID=0): 	vs MDL (w/o neural net) 13.20
(ID=0): 	22800 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1181
(ID=0): Epoch 22850 Loss 7.29
(ID=0): 	vs MDL (w/o neural net) 12.61
(ID=0): 	22850 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1039
(ID=0): Epoch 22900 Loss 8.29
(ID=0): 	vs MDL (w/o neural net) 13.37
(ID=0): 	22900 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1122
Sampling 1000 programs from the prior on 4 CPUs...
Got 846/1000 valid samples.
(ID=0): Epoch 22950 Loss 6.56
(ID=0): 	vs MDL (w/o neural net) 12.04
(ID=0): 	22950 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0915
(ID=0): Epoch 23000 Loss 8.29
(ID=0): 	vs MDL (w/o neural net) 12.76
(ID=0): 	23000 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1124
(ID=0): Epoch 23050 Loss 7.14
(ID=0): 	vs MDL (w/o neural net) 12.27
(ID=0): 	23050 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1083
(ID=0): Epoch 23100 Loss 8.10
(ID=0): 	vs MDL (w/o neural net) 13.38
(ID=0): 	23100 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1053
(ID=0): Epoch 23150 Loss 7.34
(ID=0): 	vs MDL (w/o neural net) 12.15
(ID=0): 	23150 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0983
(ID=0): Epoch 23200 Loss 8.53
(ID=0): 	vs MDL (w/o neural net) 13.31
(ID=0): 	23200 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1126
(ID=0): Epoch 23250 Loss 7.88
(ID=0): 	vs MDL (w/o neural net) 13.11
(ID=0): 	23250 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1068
(ID=0): Epoch 23300 Loss 7.65
(ID=0): 	vs MDL (w/o neural net) 12.72
(ID=0): 	23300 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1078
(ID=0): Epoch 23350 Loss 7.63
(ID=0): 	vs MDL (w/o neural net) 12.83
(ID=0): 	23350 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1059
(ID=0): Epoch 23400 Loss 8.20
(ID=0): 	vs MDL (w/o neural net) 13.37
(ID=0): 	23400 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1083
(ID=0): Epoch 23450 Loss 7.86
(ID=0): 	vs MDL (w/o neural net) 12.94
(ID=0): 	23450 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1115
(ID=0): Epoch 23500 Loss 7.78
(ID=0): 	vs MDL (w/o neural net) 12.58
(ID=0): 	23500 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1098
(ID=0): Epoch 23550 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.16
(ID=0): 	23550 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1099
(ID=0): Epoch 23600 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 12.91
(ID=0): 	23600 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1241
(ID=0): Epoch 23650 Loss 8.44
(ID=0): 	vs MDL (w/o neural net) 13.84
(ID=0): 	23650 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1189
(ID=0): Epoch 23700 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.50
(ID=0): 	23700 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1195
(ID=0): Epoch 23750 Loss 7.97
(ID=0): 	vs MDL (w/o neural net) 12.80
(ID=0): 	23750 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1118
Sampling 1000 programs from the prior on 4 CPUs...
Got 857/1000 valid samples.
(ID=0): Epoch 23800 Loss 7.72
(ID=0): 	vs MDL (w/o neural net) 13.21
(ID=0): 	23800 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1122
(ID=0): Epoch 23850 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 13.20
(ID=0): 	23850 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1130
(ID=0): Epoch 23900 Loss 7.11
(ID=0): 	vs MDL (w/o neural net) 12.41
(ID=0): 	23900 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1035
(ID=0): Epoch 23950 Loss 7.52
(ID=0): 	vs MDL (w/o neural net) 13.07
(ID=0): 	23950 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1059
(ID=0): Epoch 24000 Loss 8.68
(ID=0): 	vs MDL (w/o neural net) 13.51
(ID=0): 	24000 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1234
(ID=0): Epoch 24050 Loss 7.94
(ID=0): 	vs MDL (w/o neural net) 12.75
(ID=0): 	24050 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1152
(ID=0): Epoch 24100 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.04
(ID=0): 	24100 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1231
(ID=0): Epoch 24150 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 12.80
(ID=0): 	24150 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1137
(ID=0): Epoch 24200 Loss 8.56
(ID=0): 	vs MDL (w/o neural net) 13.73
(ID=0): 	24200 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1178
(ID=0): Epoch 24250 Loss 7.85
(ID=0): 	vs MDL (w/o neural net) 12.64
(ID=0): 	24250 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1102
(ID=0): Epoch 24300 Loss 6.83
(ID=0): 	vs MDL (w/o neural net) 11.89
(ID=0): 	24300 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0978
(ID=0): Epoch 24350 Loss 7.24
(ID=0): 	vs MDL (w/o neural net) 12.59
(ID=0): 	24350 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1040
(ID=0): Epoch 24400 Loss 6.96
(ID=0): 	vs MDL (w/o neural net) 12.17
(ID=0): 	24400 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0999
(ID=0): Epoch 24450 Loss 7.97
(ID=0): 	vs MDL (w/o neural net) 12.84
(ID=0): 	24450 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1090
(ID=0): Epoch 24500 Loss 7.55
(ID=0): 	vs MDL (w/o neural net) 13.14
(ID=0): 	24500 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1089
(ID=0): Epoch 24550 Loss 7.18
(ID=0): 	vs MDL (w/o neural net) 12.32
(ID=0): 	24550 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.0984
(ID=0): Epoch 24600 Loss 7.26
(ID=0): 	vs MDL (w/o neural net) 12.31
(ID=0): 	24600 cum grad steps. 36.3 steps/sec | 81-way aux classif loss 0.1077
Sampling 1000 programs from the prior on 4 CPUs...
Got 836/1000 valid samples.
(ID=0): Epoch 24650 Loss 7.40
(ID=0): 	vs MDL (w/o neural net) 12.63
(ID=0): 	24650 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1004
(ID=0): Epoch 24700 Loss 7.56
(ID=0): 	vs MDL (w/o neural net) 12.69
(ID=0): 	24700 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1063
(ID=0): Epoch 24750 Loss 7.58
(ID=0): 	vs MDL (w/o neural net) 12.53
(ID=0): 	24750 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1021
(ID=0): Epoch 24800 Loss 7.19
(ID=0): 	vs MDL (w/o neural net) 12.07
(ID=0): 	24800 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1003
(ID=0): Epoch 24850 Loss 7.71
(ID=0): 	vs MDL (w/o neural net) 12.83
(ID=0): 	24850 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1052
(ID=0): Epoch 24900 Loss 7.69
(ID=0): 	vs MDL (w/o neural net) 12.57
(ID=0): 	24900 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1088
(ID=0): Epoch 24950 Loss 6.98
(ID=0): 	vs MDL (w/o neural net) 12.17
(ID=0): 	24950 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1003
(ID=0): Epoch 25000 Loss 7.81
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	25000 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1182
(ID=0): Epoch 25050 Loss 7.71
(ID=0): 	vs MDL (w/o neural net) 13.21
(ID=0): 	25050 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1137
(ID=0): Epoch 25100 Loss 7.12
(ID=0): 	vs MDL (w/o neural net) 12.10
(ID=0): 	25100 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1052
(ID=0): Epoch 25150 Loss 7.92
(ID=0): 	vs MDL (w/o neural net) 13.12
(ID=0): 	25150 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1115
(ID=0): Epoch 25200 Loss 7.49
(ID=0): 	vs MDL (w/o neural net) 12.92
(ID=0): 	25200 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1059
(ID=0): Epoch 25250 Loss 7.64
(ID=0): 	vs MDL (w/o neural net) 12.72
(ID=0): 	25250 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1056
(ID=0): Epoch 25300 Loss 7.79
(ID=0): 	vs MDL (w/o neural net) 12.65
(ID=0): 	25300 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1072
(ID=0): Epoch 25350 Loss 8.05
(ID=0): 	vs MDL (w/o neural net) 12.66
(ID=0): 	25350 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1108
(ID=0): Epoch 25400 Loss 7.20
(ID=0): 	vs MDL (w/o neural net) 12.58
(ID=0): 	25400 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1005
(ID=0): Epoch 25450 Loss 7.09
(ID=0): 	vs MDL (w/o neural net) 12.17
(ID=0): 	25450 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0989
Sampling 1000 programs from the prior on 4 CPUs...
Got 835/1000 valid samples.
(ID=0): Epoch 25500 Loss 7.71
(ID=0): 	vs MDL (w/o neural net) 12.98
(ID=0): 	25500 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1063
(ID=0): Epoch 25550 Loss 6.92
(ID=0): 	vs MDL (w/o neural net) 11.84
(ID=0): 	25550 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0952
(ID=0): Epoch 25600 Loss 7.65
(ID=0): 	vs MDL (w/o neural net) 12.97
(ID=0): 	25600 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1063
(ID=0): Epoch 25650 Loss 7.58
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	25650 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1060
(ID=0): Epoch 25700 Loss 7.68
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	25700 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1072
(ID=0): Epoch 25750 Loss 7.35
(ID=0): 	vs MDL (w/o neural net) 12.28
(ID=0): 	25750 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0994
(ID=0): Epoch 25800 Loss 8.59
(ID=0): 	vs MDL (w/o neural net) 13.61
(ID=0): 	25800 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1137
(ID=0): Epoch 25850 Loss 7.84
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	25850 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1068
(ID=0): Epoch 25900 Loss 7.75
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	25900 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1040
(ID=0): Epoch 25950 Loss 8.01
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	25950 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1007
(ID=0): Epoch 26000 Loss 8.10
(ID=0): 	vs MDL (w/o neural net) 13.23
(ID=0): 	26000 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1091
(ID=0): Epoch 26050 Loss 7.66
(ID=0): 	vs MDL (w/o neural net) 12.84
(ID=0): 	26050 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1021
(ID=0): Epoch 26100 Loss 7.38
(ID=0): 	vs MDL (w/o neural net) 12.45
(ID=0): 	26100 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1005
(ID=0): Epoch 26150 Loss 8.01
(ID=0): 	vs MDL (w/o neural net) 13.17
(ID=0): 	26150 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1068
(ID=0): Epoch 26200 Loss 7.43
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	26200 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1024
(ID=0): Epoch 26250 Loss 7.09
(ID=0): 	vs MDL (w/o neural net) 12.09
(ID=0): 	26250 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0981
(ID=0): Epoch 26300 Loss 7.70
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	26300 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1052
Sampling 1000 programs from the prior on 4 CPUs...
Got 850/1000 valid samples.
(ID=0): Epoch 26350 Loss 7.40
(ID=0): 	vs MDL (w/o neural net) 12.97
(ID=0): 	26350 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0972
(ID=0): Epoch 26400 Loss 8.18
(ID=0): 	vs MDL (w/o neural net) 13.12
(ID=0): 	26400 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1054
(ID=0): Epoch 26450 Loss 7.95
(ID=0): 	vs MDL (w/o neural net) 13.06
(ID=0): 	26450 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1090
(ID=0): Epoch 26500 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 12.65
(ID=0): 	26500 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1055
(ID=0): Epoch 26550 Loss 7.51
(ID=0): 	vs MDL (w/o neural net) 12.52
(ID=0): 	26550 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1012
(ID=0): Epoch 26600 Loss 8.24
(ID=0): 	vs MDL (w/o neural net) 12.93
(ID=0): 	26600 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1130
(ID=0): Epoch 26650 Loss 8.02
(ID=0): 	vs MDL (w/o neural net) 13.26
(ID=0): 	26650 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1106
(ID=0): Epoch 26700 Loss 8.13
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	26700 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1043
(ID=0): Epoch 26750 Loss 7.76
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	26750 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1074
(ID=0): Epoch 26800 Loss 6.89
(ID=0): 	vs MDL (w/o neural net) 12.35
(ID=0): 	26800 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0941
(ID=0): Epoch 26850 Loss 7.49
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	26850 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1009
(ID=0): Epoch 26900 Loss 7.08
(ID=0): 	vs MDL (w/o neural net) 12.33
(ID=0): 	26900 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1028
(ID=0): Epoch 26950 Loss 8.05
(ID=0): 	vs MDL (w/o neural net) 13.07
(ID=0): 	26950 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1024
(ID=0): Epoch 27000 Loss 7.85
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	27000 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1066
(ID=0): Epoch 27050 Loss 7.85
(ID=0): 	vs MDL (w/o neural net) 12.98
(ID=0): 	27050 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1063
(ID=0): Epoch 27100 Loss 7.15
(ID=0): 	vs MDL (w/o neural net) 12.62
(ID=0): 	27100 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.0991
(ID=0): Epoch 27150 Loss 7.92
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	27150 cum grad steps. 36.2 steps/sec | 81-way aux classif loss 0.1083
Sampling 1000 programs from the prior on 4 CPUs...
Got 852/1000 valid samples.
(ID=0): Epoch 27200 Loss 7.84
(ID=0): 	vs MDL (w/o neural net) 12.88
(ID=0): 	27200 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1101
(ID=0): Epoch 27250 Loss 7.78
(ID=0): 	vs MDL (w/o neural net) 13.34
(ID=0): 	27250 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1023
(ID=0): Epoch 27300 Loss 7.77
(ID=0): 	vs MDL (w/o neural net) 13.05
(ID=0): 	27300 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1000
(ID=0): Epoch 27350 Loss 6.92
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	27350 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0990
(ID=0): Epoch 27400 Loss 6.48
(ID=0): 	vs MDL (w/o neural net) 11.58
(ID=0): 	27400 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0910
(ID=0): Epoch 27450 Loss 7.55
(ID=0): 	vs MDL (w/o neural net) 12.77
(ID=0): 	27450 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1022
(ID=0): Epoch 27500 Loss 7.31
(ID=0): 	vs MDL (w/o neural net) 12.30
(ID=0): 	27500 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1035
(ID=0): Epoch 27550 Loss 7.37
(ID=0): 	vs MDL (w/o neural net) 12.09
(ID=0): 	27550 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0993
(ID=0): Epoch 27600 Loss 7.53
(ID=0): 	vs MDL (w/o neural net) 12.56
(ID=0): 	27600 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1027
(ID=0): Epoch 27650 Loss 6.72
(ID=0): 	vs MDL (w/o neural net) 11.93
(ID=0): 	27650 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0916
(ID=0): Epoch 27700 Loss 7.60
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	27700 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0975
(ID=0): Epoch 27750 Loss 7.48
(ID=0): 	vs MDL (w/o neural net) 12.71
(ID=0): 	27750 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1029
(ID=0): Epoch 27800 Loss 7.29
(ID=0): 	vs MDL (w/o neural net) 12.56
(ID=0): 	27800 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1036
(ID=0): Epoch 27850 Loss 7.32
(ID=0): 	vs MDL (w/o neural net) 12.78
(ID=0): 	27850 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1019
(ID=0): Epoch 27900 Loss 7.11
(ID=0): 	vs MDL (w/o neural net) 12.45
(ID=0): 	27900 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1061
(ID=0): Epoch 27950 Loss 7.98
(ID=0): 	vs MDL (w/o neural net) 13.31
(ID=0): 	27950 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1105
(ID=0): Epoch 28000 Loss 8.24
(ID=0): 	vs MDL (w/o neural net) 13.60
(ID=0): 	28000 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1144
Sampling 1000 programs from the prior on 4 CPUs...
Got 859/1000 valid samples.
(ID=0): Epoch 28050 Loss 8.22
(ID=0): 	vs MDL (w/o neural net) 13.39
(ID=0): 	28050 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1050
(ID=0): Epoch 28100 Loss 7.49
(ID=0): 	vs MDL (w/o neural net) 12.67
(ID=0): 	28100 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1034
(ID=0): Epoch 28150 Loss 7.34
(ID=0): 	vs MDL (w/o neural net) 12.12
(ID=0): 	28150 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0991
(ID=0): Epoch 28200 Loss 7.56
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	28200 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1013
(ID=0): Epoch 28250 Loss 7.56
(ID=0): 	vs MDL (w/o neural net) 13.18
(ID=0): 	28250 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0999
(ID=0): Epoch 28300 Loss 7.69
(ID=0): 	vs MDL (w/o neural net) 12.66
(ID=0): 	28300 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1019
(ID=0): Epoch 28350 Loss 7.37
(ID=0): 	vs MDL (w/o neural net) 12.50
(ID=0): 	28350 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0970
(ID=0): Epoch 28400 Loss 8.09
(ID=0): 	vs MDL (w/o neural net) 13.65
(ID=0): 	28400 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1076
(ID=0): Epoch 28450 Loss 7.65
(ID=0): 	vs MDL (w/o neural net) 13.41
(ID=0): 	28450 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1033
(ID=0): Epoch 28500 Loss 8.07
(ID=0): 	vs MDL (w/o neural net) 13.36
(ID=0): 	28500 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1044
(ID=0): Epoch 28550 Loss 7.69
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	28550 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1016
(ID=0): Epoch 28600 Loss 7.32
(ID=0): 	vs MDL (w/o neural net) 12.48
(ID=0): 	28600 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1001
(ID=0): Epoch 28650 Loss 7.65
(ID=0): 	vs MDL (w/o neural net) 13.18
(ID=0): 	28650 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1033
(ID=0): Epoch 28700 Loss 7.36
(ID=0): 	vs MDL (w/o neural net) 12.46
(ID=0): 	28700 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0974
(ID=0): Epoch 28750 Loss 8.21
(ID=0): 	vs MDL (w/o neural net) 13.56
(ID=0): 	28750 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1085
(ID=0): Epoch 28800 Loss 7.97
(ID=0): 	vs MDL (w/o neural net) 13.59
(ID=0): 	28800 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1067
(ID=0): Epoch 28850 Loss 7.61
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	28850 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1006
Sampling 1000 programs from the prior on 4 CPUs...
Got 840/1000 valid samples.
(ID=0): Epoch 28900 Loss 8.00
(ID=0): 	vs MDL (w/o neural net) 12.85
(ID=0): 	28900 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1067
(ID=0): Epoch 28950 Loss 8.59
(ID=0): 	vs MDL (w/o neural net) 13.61
(ID=0): 	28950 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1132
(ID=0): Epoch 29000 Loss 7.96
(ID=0): 	vs MDL (w/o neural net) 13.30
(ID=0): 	29000 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1048
(ID=0): Epoch 29050 Loss 8.38
(ID=0): 	vs MDL (w/o neural net) 13.75
(ID=0): 	29050 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1102
(ID=0): Epoch 29100 Loss 8.13
(ID=0): 	vs MDL (w/o neural net) 13.41
(ID=0): 	29100 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1052
(ID=0): Epoch 29150 Loss 7.66
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	29150 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0992
(ID=0): Epoch 29200 Loss 7.24
(ID=0): 	vs MDL (w/o neural net) 12.35
(ID=0): 	29200 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0963
(ID=0): Epoch 29250 Loss 7.20
(ID=0): 	vs MDL (w/o neural net) 12.11
(ID=0): 	29250 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0995
(ID=0): Epoch 29300 Loss 7.08
(ID=0): 	vs MDL (w/o neural net) 12.01
(ID=0): 	29300 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1011
(ID=0): Epoch 29350 Loss 7.10
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	29350 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0985
(ID=0): Epoch 29400 Loss 7.56
(ID=0): 	vs MDL (w/o neural net) 13.16
(ID=0): 	29400 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1043
(ID=0): Epoch 29450 Loss 7.64
(ID=0): 	vs MDL (w/o neural net) 12.94
(ID=0): 	29450 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1004
(ID=0): Epoch 29500 Loss 7.66
(ID=0): 	vs MDL (w/o neural net) 12.92
(ID=0): 	29500 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1029
(ID=0): Epoch 29550 Loss 7.75
(ID=0): 	vs MDL (w/o neural net) 12.63
(ID=0): 	29550 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1056
(ID=0): Epoch 29600 Loss 7.64
(ID=0): 	vs MDL (w/o neural net) 12.39
(ID=0): 	29600 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0990
(ID=0): Epoch 29650 Loss 7.32
(ID=0): 	vs MDL (w/o neural net) 12.22
(ID=0): 	29650 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.0983
(ID=0): Epoch 29700 Loss 7.45
(ID=0): 	vs MDL (w/o neural net) 12.48
(ID=0): 	29700 cum grad steps. 36.1 steps/sec | 81-way aux classif loss 0.1054
Sampling 1000 programs from the prior on 4 CPUs...
Got 857/1000 valid samples.
(ID=0): Epoch 29750 Loss 7.23
(ID=0): 	vs MDL (w/o neural net) 12.45
(ID=0): 	29750 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0996
(ID=0): Epoch 29800 Loss 8.23
(ID=0): 	vs MDL (w/o neural net) 13.49
(ID=0): 	29800 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1078
(ID=0): Epoch 29850 Loss 8.17
(ID=0): 	vs MDL (w/o neural net) 13.23
(ID=0): 	29850 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1072
(ID=0): Epoch 29900 Loss 7.11
(ID=0): 	vs MDL (w/o neural net) 12.45
(ID=0): 	29900 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0901
(ID=0): Epoch 29950 Loss 7.77
(ID=0): 	vs MDL (w/o neural net) 13.34
(ID=0): 	29950 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1073
(ID=0): Epoch 30000 Loss 7.87
(ID=0): 	vs MDL (w/o neural net) 12.73
(ID=0): 	30000 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1044
(ID=0): Epoch 30050 Loss 7.80
(ID=0): 	vs MDL (w/o neural net) 12.73
(ID=0): 	30050 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1071
(ID=0): Epoch 30100 Loss 7.16
(ID=0): 	vs MDL (w/o neural net) 12.82
(ID=0): 	30100 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1005
(ID=0): Epoch 30150 Loss 7.39
(ID=0): 	vs MDL (w/o neural net) 12.86
(ID=0): 	30150 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1023
(ID=0): Epoch 30200 Loss 7.45
(ID=0): 	vs MDL (w/o neural net) 12.56
(ID=0): 	30200 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1003
(ID=0): Epoch 30250 Loss 7.75
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	30250 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1034
(ID=0): Epoch 30300 Loss 7.22
(ID=0): 	vs MDL (w/o neural net) 12.50
(ID=0): 	30300 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0975
(ID=0): Epoch 30350 Loss 7.63
(ID=0): 	vs MDL (w/o neural net) 13.32
(ID=0): 	30350 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1042
(ID=0): Epoch 30400 Loss 7.75
(ID=0): 	vs MDL (w/o neural net) 12.93
(ID=0): 	30400 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1032
(ID=0): Epoch 30450 Loss 7.74
(ID=0): 	vs MDL (w/o neural net) 12.80
(ID=0): 	30450 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1008
(ID=0): Epoch 30500 Loss 8.00
(ID=0): 	vs MDL (w/o neural net) 13.27
(ID=0): 	30500 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1095
(ID=0): Epoch 30550 Loss 7.16
(ID=0): 	vs MDL (w/o neural net) 12.79
(ID=0): 	30550 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0969
Sampling 1000 programs from the prior on 4 CPUs...
Got 839/1000 valid samples.
(ID=0): Epoch 30600 Loss 8.36
(ID=0): 	vs MDL (w/o neural net) 13.21
(ID=0): 	30600 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1070
(ID=0): Epoch 30650 Loss 7.77
(ID=0): 	vs MDL (w/o neural net) 12.67
(ID=0): 	30650 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1065
(ID=0): Epoch 30700 Loss 6.50
(ID=0): 	vs MDL (w/o neural net) 11.69
(ID=0): 	30700 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0902
(ID=0): Epoch 30750 Loss 7.22
(ID=0): 	vs MDL (w/o neural net) 12.26
(ID=0): 	30750 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0963
(ID=0): Epoch 30800 Loss 7.51
(ID=0): 	vs MDL (w/o neural net) 12.50
(ID=0): 	30800 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0961
(ID=0): Epoch 30850 Loss 8.38
(ID=0): 	vs MDL (w/o neural net) 13.39
(ID=0): 	30850 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1097
(ID=0): Epoch 30900 Loss 8.01
(ID=0): 	vs MDL (w/o neural net) 13.38
(ID=0): 	30900 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1065
(ID=0): Epoch 30950 Loss 7.39
(ID=0): 	vs MDL (w/o neural net) 12.11
(ID=0): 	30950 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0946
(ID=0): Epoch 31000 Loss 7.33
(ID=0): 	vs MDL (w/o neural net) 12.62
(ID=0): 	31000 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0982
(ID=0): Epoch 31050 Loss 7.52
(ID=0): 	vs MDL (w/o neural net) 12.44
(ID=0): 	31050 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0994
(ID=0): Epoch 31100 Loss 7.89
(ID=0): 	vs MDL (w/o neural net) 13.77
(ID=0): 	31100 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1085
(ID=0): Epoch 31150 Loss 7.58
(ID=0): 	vs MDL (w/o neural net) 12.96
(ID=0): 	31150 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1000
(ID=0): Epoch 31200 Loss 7.47
(ID=0): 	vs MDL (w/o neural net) 12.49
(ID=0): 	31200 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1069
(ID=0): Epoch 31250 Loss 7.60
(ID=0): 	vs MDL (w/o neural net) 12.59
(ID=0): 	31250 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0976
(ID=0): Epoch 31300 Loss 7.30
(ID=0): 	vs MDL (w/o neural net) 12.55
(ID=0): 	31300 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.1022
(ID=0): Epoch 31350 Loss 6.97
(ID=0): 	vs MDL (w/o neural net) 12.02
(ID=0): 	31350 cum grad steps. 36.0 steps/sec | 81-way aux classif loss 0.0944
Sampling 1000 programs from the prior on 4 CPUs...
Got 850/1000 valid samples.
(ID=0): Epoch 31400 Loss 7.78
(ID=0): 	vs MDL (w/o neural net) 13.56
(ID=0): 	31400 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0998
(ID=0): Epoch 31450 Loss 9.11
(ID=0): 	vs MDL (w/o neural net) 13.87
(ID=0): 	31450 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1190
(ID=0): Epoch 31500 Loss 7.33
(ID=0): 	vs MDL (w/o neural net) 12.26
(ID=0): 	31500 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0968
(ID=0): Epoch 31550 Loss 7.80
(ID=0): 	vs MDL (w/o neural net) 12.64
(ID=0): 	31550 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1066
(ID=0): Epoch 31600 Loss 7.15
(ID=0): 	vs MDL (w/o neural net) 12.29
(ID=0): 	31600 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0973
(ID=0): Epoch 31650 Loss 7.54
(ID=0): 	vs MDL (w/o neural net) 12.35
(ID=0): 	31650 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0999
(ID=0): Epoch 31700 Loss 7.59
(ID=0): 	vs MDL (w/o neural net) 12.89
(ID=0): 	31700 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1020
(ID=0): Epoch 31750 Loss 7.49
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	31750 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1003
(ID=0): Epoch 31800 Loss 7.23
(ID=0): 	vs MDL (w/o neural net) 12.56
(ID=0): 	31800 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0999
(ID=0): Epoch 31850 Loss 7.82
(ID=0): 	vs MDL (w/o neural net) 13.04
(ID=0): 	31850 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1037
(ID=0): Epoch 31900 Loss 7.98
(ID=0): 	vs MDL (w/o neural net) 13.12
(ID=0): 	31900 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1007
(ID=0): Epoch 31950 Loss 7.42
(ID=0): 	vs MDL (w/o neural net) 12.59
(ID=0): 	31950 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0965
(ID=0): Epoch 32000 Loss 7.83
(ID=0): 	vs MDL (w/o neural net) 13.06
(ID=0): 	32000 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1040
(ID=0): Epoch 32050 Loss 6.77
(ID=0): 	vs MDL (w/o neural net) 12.29
(ID=0): 	32050 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0951
(ID=0): Epoch 32100 Loss 7.33
(ID=0): 	vs MDL (w/o neural net) 12.90
(ID=0): 	32100 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0996
(ID=0): Epoch 32150 Loss 7.87
(ID=0): 	vs MDL (w/o neural net) 12.74
(ID=0): 	32150 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0993
(ID=0): Epoch 32200 Loss 7.53
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	32200 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0966
Sampling 1000 programs from the prior on 4 CPUs...
Got 857/1000 valid samples.
(ID=0): Epoch 32250 Loss 8.00
(ID=0): 	vs MDL (w/o neural net) 12.95
(ID=0): 	32250 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1031
(ID=0): Epoch 32300 Loss 8.15
(ID=0): 	vs MDL (w/o neural net) 13.59
(ID=0): 	32300 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1014
(ID=0): Epoch 32350 Loss 8.11
(ID=0): 	vs MDL (w/o neural net) 13.17
(ID=0): 	32350 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1095
(ID=0): Epoch 32400 Loss 8.19
(ID=0): 	vs MDL (w/o neural net) 13.03
(ID=0): 	32400 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1083
(ID=0): Epoch 32450 Loss 6.88
(ID=0): 	vs MDL (w/o neural net) 12.07
(ID=0): 	32450 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0956
(ID=0): Epoch 32500 Loss 7.84
(ID=0): 	vs MDL (w/o neural net) 12.54
(ID=0): 	32500 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1007
(ID=0): Epoch 32550 Loss 7.54
(ID=0): 	vs MDL (w/o neural net) 12.59
(ID=0): 	32550 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1008
(ID=0): Epoch 32600 Loss 8.14
(ID=0): 	vs MDL (w/o neural net) 13.00
(ID=0): 	32600 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1041
(ID=0): Epoch 32650 Loss 7.75
(ID=0): 	vs MDL (w/o neural net) 12.87
(ID=0): 	32650 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1478
(ID=0): Epoch 32700 Loss 7.39
(ID=0): 	vs MDL (w/o neural net) 13.08
(ID=0): 	32700 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1030
(ID=0): Epoch 32750 Loss 8.04
(ID=0): 	vs MDL (w/o neural net) 13.21
(ID=0): 	32750 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1034
(ID=0): Epoch 32800 Loss 7.64
(ID=0): 	vs MDL (w/o neural net) 12.94
(ID=0): 	32800 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1034
(ID=0): Epoch 32850 Loss 8.25
(ID=0): 	vs MDL (w/o neural net) 13.56
(ID=0): 	32850 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1143
(ID=0): Epoch 32900 Loss 7.16
(ID=0): 	vs MDL (w/o neural net) 12.77
(ID=0): 	32900 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1015
(ID=0): Epoch 32950 Loss 7.63
(ID=0): 	vs MDL (w/o neural net) 12.76
(ID=0): 	32950 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1014
(ID=0): Epoch 33000 Loss 8.49
(ID=0): 	vs MDL (w/o neural net) 13.68
(ID=0): 	33000 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1101
(ID=0): Epoch 33050 Loss 6.81
(ID=0): 	vs MDL (w/o neural net) 12.38
(ID=0): 	33050 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0952
(ID=0): Epoch 33100 Loss 8.16
(ID=0): 	vs MDL (w/o neural net) 12.79
(ID=0): 	33100 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1144
Sampling 1000 programs from the prior on 4 CPUs...
Got 868/1000 valid samples.
(ID=0): Epoch 33150 Loss 7.47
(ID=0): 	vs MDL (w/o neural net) 12.36
(ID=0): 	33150 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1067
(ID=0): Epoch 33200 Loss 7.89
(ID=0): 	vs MDL (w/o neural net) 13.02
(ID=0): 	33200 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1085
(ID=0): Epoch 33250 Loss 7.34
(ID=0): 	vs MDL (w/o neural net) 12.64
(ID=0): 	33250 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1019
(ID=0): Epoch 33300 Loss 7.69
(ID=0): 	vs MDL (w/o neural net) 13.27
(ID=0): 	33300 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1035
(ID=0): Epoch 33350 Loss 8.20
(ID=0): 	vs MDL (w/o neural net) 13.64
(ID=0): 	33350 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1120
(ID=0): Epoch 33400 Loss 6.94
(ID=0): 	vs MDL (w/o neural net) 12.70
(ID=0): 	33400 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0984
(ID=0): Epoch 33450 Loss 7.15
(ID=0): 	vs MDL (w/o neural net) 12.65
(ID=0): 	33450 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0967
(ID=0): Epoch 33500 Loss 7.96
(ID=0): 	vs MDL (w/o neural net) 13.58
(ID=0): 	33500 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1103
(ID=0): Epoch 33550 Loss 8.49
(ID=0): 	vs MDL (w/o neural net) 13.60
(ID=0): 	33550 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1120
(ID=0): Epoch 33600 Loss 7.44
(ID=0): 	vs MDL (w/o neural net) 12.81
(ID=0): 	33600 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0974
(ID=0): Epoch 33650 Loss 7.79
(ID=0): 	vs MDL (w/o neural net) 13.21
(ID=0): 	33650 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1029
(ID=0): Epoch 33700 Loss 7.55
(ID=0): 	vs MDL (w/o neural net) 12.31
(ID=0): 	33700 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1082
(ID=0): Epoch 33750 Loss 8.93
(ID=0): 	vs MDL (w/o neural net) 13.47
(ID=0): 	33750 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1137
(ID=0): Epoch 33800 Loss 8.28
(ID=0): 	vs MDL (w/o neural net) 13.77
(ID=0): 	33800 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1079
(ID=0): Epoch 33850 Loss 6.77
(ID=0): 	vs MDL (w/o neural net) 12.35
(ID=0): 	33850 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0966
(ID=0): Epoch 33900 Loss 8.92
(ID=0): 	vs MDL (w/o neural net) 13.92
(ID=0): 	33900 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1152
(ID=0): Epoch 33950 Loss 7.51
(ID=0): 	vs MDL (w/o neural net) 12.56
(ID=0): 	33950 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1043
Sampling 1000 programs from the prior on 4 CPUs...
Got 864/1000 valid samples.
(ID=0): Epoch 34000 Loss 7.48
(ID=0): 	vs MDL (w/o neural net) 12.48
(ID=0): 	34000 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0966
(ID=0): Epoch 34050 Loss 8.52
(ID=0): 	vs MDL (w/o neural net) 13.32
(ID=0): 	34050 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1126
(ID=0): Epoch 34100 Loss 8.27
(ID=0): 	vs MDL (w/o neural net) 13.45
(ID=0): 	34100 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1053
(ID=0): Epoch 34150 Loss 6.89
(ID=0): 	vs MDL (w/o neural net) 12.02
(ID=0): 	34150 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0925
(ID=0): Epoch 34200 Loss 8.18
(ID=0): 	vs MDL (w/o neural net) 13.42
(ID=0): 	34200 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1112
(ID=0): Epoch 34250 Loss 7.57
(ID=0): 	vs MDL (w/o neural net) 12.75
(ID=0): 	34250 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1016
(ID=0): Epoch 34300 Loss 8.18
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	34300 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1121
(ID=0): Epoch 34350 Loss 7.52
(ID=0): 	vs MDL (w/o neural net) 12.74
(ID=0): 	34350 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1036
(ID=0): Epoch 34400 Loss 7.47
(ID=0): 	vs MDL (w/o neural net) 12.48
(ID=0): 	34400 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1019
(ID=0): Epoch 34450 Loss 7.87
(ID=0): 	vs MDL (w/o neural net) 13.32
(ID=0): 	34450 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1092
(ID=0): Epoch 34500 Loss 7.33
(ID=0): 	vs MDL (w/o neural net) 12.84
(ID=0): 	34500 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1004
(ID=0): Epoch 34550 Loss 7.21
(ID=0): 	vs MDL (w/o neural net) 12.73
(ID=0): 	34550 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0987
(ID=0): Epoch 34600 Loss 7.91
(ID=0): 	vs MDL (w/o neural net) 12.97
(ID=0): 	34600 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1065
(ID=0): Epoch 34650 Loss 6.29
(ID=0): 	vs MDL (w/o neural net) 11.67
(ID=0): 	34650 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0888
(ID=0): Epoch 34700 Loss 6.77
(ID=0): 	vs MDL (w/o neural net) 12.34
(ID=0): 	34700 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.0993
(ID=0): Epoch 34750 Loss 8.02
(ID=0): 	vs MDL (w/o neural net) 12.58
(ID=0): 	34750 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1038
(ID=0): Epoch 34800 Loss 7.54
(ID=0): 	vs MDL (w/o neural net) 12.85
(ID=0): 	34800 cum grad steps. 35.9 steps/sec | 81-way aux classif loss 0.1021
Sampling 1000 programs from the prior on 4 CPUs...
Got 856/1000 valid samples.
(ID=0): Epoch 34850 Loss 8.36
(ID=0): 	vs MDL (w/o neural net) 13.60
(ID=0): 	34850 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1072
(ID=0): Epoch 34900 Loss 7.47
(ID=0): 	vs MDL (w/o neural net) 12.95
(ID=0): 	34900 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1052
(ID=0): Epoch 34950 Loss 8.17
(ID=0): 	vs MDL (w/o neural net) 13.09
(ID=0): 	34950 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1062
(ID=0): Epoch 35000 Loss 7.86
(ID=0): 	vs MDL (w/o neural net) 13.34
(ID=0): 	35000 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1029
(ID=0): Epoch 35050 Loss 7.90
(ID=0): 	vs MDL (w/o neural net) 12.91
(ID=0): 	35050 cum grad steps. 35.8 steps/sec | 81-way aux classif loss 0.1049
